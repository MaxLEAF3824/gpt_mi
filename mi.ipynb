{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T12:12:21.863130Z",
     "start_time": "2024-03-21T12:12:18.025864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ââ©‰ΩôÂÜÖÂ≠ò: 847.0 G\n",
      "ÂΩìÂâç‰∏ªÊú∫ÂêçÊòØ:SH-IDC1-10-140-0-212\n",
      "SH-IDC1-10-140-0-212      Thu Mar 21 20:12:21 2024  525.60.13\n",
      "[0] NVIDIA A100-SXM4-80GB | 36¬∞C,  52 % | 76620 / 81920 MB | gaopeng(75708M) hezexin(27M) hezexin(38M) hezexin(45M) hezexin(6M) hezexin(34M) hezexin(18M) hezexin(45M) hezexin(40M) hezexin(43M) hezexin(36M) hezexin(19M) hezexin(128M) hezexin(19M) hezexin(31M) hezexin(6M) hezexin(34M) hezexin(37M) hezexin(37M) hezexin(62M) hezexin(13M) hezexin(54M) hezexin(28M) hezexin(45M) hezexin(35M)\n",
      "[1] NVIDIA A100-SXM4-80GB | 31¬∞C,   0 % |  5913 / 81920 MB | yangyue(5910M)\n",
      "[2] NVIDIA A100-SXM4-80GB | 30¬∞C,   0 % |  5913 / 81920 MB | yangyue(5910M)\n",
      "[3] NVIDIA A100-SXM4-80GB | 30¬∞C,   0 % |  2621 / 81920 MB | yangyue(2618M)\n",
      "[4] NVIDIA A100-SXM4-80GB | 29¬∞C,   0 % |  5913 / 81920 MB | yangyue(5910M)\n",
      "[5] NVIDIA A100-SXM4-80GB | 37¬∞C,  53 % | 75711 / 81920 MB | gaopeng(75708M)\n",
      "[6] NVIDIA A100-SXM4-80GB | 32¬∞C,   0 % |  5913 / 81920 MB | yangyue(5910M)\n",
      "[7] NVIDIA A100-SXM4-80GB | 46¬∞C,  91 % | 48673 / 81920 MB | guoyiqiu(48670M)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_DATASETS_OFFLINE']= \"1\"\n",
    "os.environ['TRANSFORMERS_OFFLINE']= \"1\"\n",
    "import transformer_lens\n",
    "import datasets\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, Features, Array2D, Array3D\n",
    "from typing import List, Tuple, Union\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import rouge_score\n",
    "from rouge_score import rouge_scorer\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from copy import deepcopy\n",
    "import re\n",
    "datasets.set_caching_enabled(False)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def print_sys_info():\n",
    "    import psutil\n",
    "    import socket\n",
    "    import gpustat\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(\"Ââ©‰ΩôÂÜÖÂ≠ò: {} G\".format(memory.available / 1024 / 1024 // 1024))\n",
    "    host_name = socket.gethostname()\n",
    "    print(f\"ÂΩìÂâç‰∏ªÊú∫ÂêçÊòØ:{host_name}\")\n",
    "    gpustat.print_gpustat()\n",
    "\n",
    "\n",
    "def launch_clash():\n",
    "    import subprocess\n",
    "    import os\n",
    "\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if not result.stdout:\n",
    "        subprocess.Popen(\"~/tools/clash/clash\", shell=True)\n",
    "        result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(f\"Clash is running, pid: {result.stdout}\")\n",
    "    os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "    os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "def close_clash():\n",
    "    import subprocess\n",
    "    result = subprocess.run(\"killall clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(result.stdout)\n",
    "    !unset http_proxy\n",
    "    !unset https_proxy\n",
    "\n",
    "print_sys_info()\n",
    "# launch_clash()\n",
    "# close_clash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:47:10.282734Z",
     "start_time": "2024-03-19T10:46:53.059691Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1c2a8d757f74656b7e6f6c6095e89e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gemma-2b into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "hf_model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = HookedTransformer.from_pretrained(\"gemma-2b\", hf_model=hf_model, tokenizer=hf_tokenizer, default_padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-19T10:48:51.653391Z",
     "start_time": "2024-03-19T10:48:42.044433Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[     2,  17534,   2134,    575,    498,   1641,    109,    651,   2412,\n",
      "            603,    476,    585,   1641],\n",
      "        [     2,  17534,   2134, 235269,   1368,    708,    692,   3900,   3646,\n",
      "         235336,    109, 235285,   1144],\n",
      "        [     0,      2,  17534,   1104, 235341, 169692,    109,   1718, 235303,\n",
      "         235256,  10920,    577,   4675],\n",
      "        [     0,      2,  17534, 235269,   1368,    798,    590,   5422,    692,\n",
      "           3646, 235336,    109, 235285]])\n",
      "['<bos>hello world in c++\\n\\nThe following is a C++', '<bos>hello world, how are you doing today?\\n\\nI am', \"<pad><bos>hello there! üëã\\n\\nIt's wonderful to hear\", '<pad><bos>hello, how can I assist you today?\\n\\nI']\n"
     ]
    }
   ],
   "source": [
    "hf_tokenizer.padding_side = \"left\"\n",
    "outputs_tokenized = hf_model.generate(hf_tokenizer([\"hello world\",\"hello\"], padding=True, return_tensors=\"pt\").input_ids, do_sample=True, max_new_tokens=10, num_return_sequences=2, temperature=0.9)\n",
    "print(outputs_tokenized)\n",
    "print(hf_tokenizer.batch_decode(outputs_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sst2\")['validation']\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prompt_tmp = \"Determine whether the sentiment of this sentence is Positive or Negative.\\nSentence:{}\\nSentiment:\"\n",
    "tagger = lambda x: \" Positive\" if x == 1 else \" Negative\"\n",
    "test_dst = [(prompt_tmp.format(dataset[i]['sentence']), tagger(dataset[i]['label'])) for i in range(len(dataset))]\n",
    "for (question, answer) in test_dst[:1]:\n",
    "    print(question)\n",
    "    utils.test_prompt(question, answer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def single_head_ablation(model: HookedTransformer, layer_idx, head_idx, all_cache: Float[torch.Tensor, \"l p h d\"]):\n",
    "    model.set_use_attn_result(True)\n",
    "\n",
    "    def edit_hook(\n",
    "            result: Float[torch.Tensor, \"b p h d\"],\n",
    "            hook: HookPoint,\n",
    "    ) -> Float[torch.Tensor, \"b p h d\"]:\n",
    "        layer_idx = int(hook.name.split('.')[1])\n",
    "        seq_len = result.shape[1]\n",
    "        result[:, :, head_idx, :] = einops.repeat(all_cache[layer_idx, :seq_len, head_idx, :], 'p d -> b p d', b=result.shape[0]).to(result.device)\n",
    "        return result\n",
    "\n",
    "    model.add_hook(utils.get_act_name('result', layer_idx), edit_hook)\n",
    "    return model\n",
    "\n",
    "def single_pattern_ablation(model: HookedTransformer, layer_idx, head_idx, query_idx, key_idx):\n",
    "    def edit_hook(\n",
    "            pattern: Float[torch.Tensor, \"batch, head_index, query_pos, key_pos\"],\n",
    "            hook: HookPoint,\n",
    "    ) -> Float[torch.Tensor, \"batch, head_index, query_pos, key_pos\"]:\n",
    "        pattern[:, head_idx, query_idx, :] = 0\n",
    "        pattern[:, head_idx, query_idx, key_idx] = 1\n",
    "        return pattern\n",
    "\n",
    "    model.add_hook(utils.get_act_name('pattern', layer_idx), edit_hook)\n",
    "    return model\n",
    "\n",
    "def get_random_head_cache(prompt_template='{}', sample_length=200, sample_size=64, seed=42):\n",
    "    model.set_use_attn_result(True)\n",
    "    all_tokens = model.to_str_tokens(torch.arange(0, model.tokenizer.vocab_size, dtype=torch.long))\n",
    "    valid_tokens = [t for t in all_tokens if t.startswith(\" \")]\n",
    "    all_prompt = []\n",
    "    for i in range(sample_size):\n",
    "        random.seed(seed + i)\n",
    "        random_tokens = random.choices(valid_tokens, k=sample_length)\n",
    "        all_prompt.append(prompt_template.format(''.join(random_tokens)))\n",
    "    all_cache = {}\n",
    "    batch_size = 4\n",
    "    batched_prompt = [all_prompt[i:i + batch_size] for i in range(0, len(all_prompt), batch_size)]\n",
    "    for batch_prompt in batched_prompt:\n",
    "        batch_output, batch_cache = model.run_with_cache(batch_prompt, names_filter=lambda x: 'result' in x, device='cpu', padding_side='right')\n",
    "        for k in batch_cache.keys():\n",
    "            all_cache[k] = all_cache.get(k, []) + [batch_cache[k]]\n",
    "    all_cache = dict(sorted(all_cache.items(), key=lambda x: int(x[0].split('.')[1])))\n",
    "    final_cache = []\n",
    "    for v in all_cache.values():\n",
    "        v = [c[:,:sample_length,...] for c in v]\n",
    "        final_cache.append(torch.vstack(v).mean(0).unsqueeze(0))\n",
    "    final_cache = torch.cat(final_cache, dim=0)\n",
    "    # all_cache = torch.cat([torch.vstack(v).mean(0).unsqueeze(0) for v in all_cache.values()], dim=0)  # [layer pos head_idx d_model]\n",
    "    return final_cache\n",
    "\n",
    "def full_logit_lens(model: HookedTransformer, prompt: Union[str, List[str]], target_token: Union[str, List[str]], token_idx=-1, k=8):\n",
    "    model.set_use_attn_result(True)\n",
    "    prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "    prompt_id = model.to_tokens(prompt, padding_side='left')\n",
    "    target_token = [target_token] if isinstance(target_token, str) else target_token\n",
    "    target_id = model.to_tokens(target_token, prepend_bos=False)\n",
    "    logit_lens = torch.zeros((model.cfg.n_heads + 5, model.cfg.n_layers))\n",
    "    residual_loc = dict(\n",
    "        hook_result=(0, model.cfg.n_heads),\n",
    "        hook_resid_pre=(model.cfg.n_heads + 0, model.cfg.n_heads + 1),\n",
    "        hook_resid_mid=(model.cfg.n_heads + 1, model.cfg.n_heads + 2),\n",
    "        hook_resid_post=(model.cfg.n_heads + 2, model.cfg.n_heads + 3),\n",
    "        hook_attn_out=(model.cfg.n_heads + 3, model.cfg.n_heads + 4),\n",
    "        hook_mlp_out=(model.cfg.n_heads + 4, model.cfg.n_heads + 5),\n",
    "    )\n",
    "    annotations = []\n",
    "\n",
    "    def logit_lens_hook(\n",
    "            residual: Union[Float[torch.Tensor, \"b p d\"], Float[torch.Tensor, \"b p h d\"]],\n",
    "            hook: HookPoint,\n",
    "    ):\n",
    "        layer_idx = int(hook.name.split('.')[1])\n",
    "        hook_name = hook.name.split('.')[-1]\n",
    "        loc = residual_loc[hook_name]\n",
    "        residual_clone = residual.clone()\n",
    "        if residual_clone.dim() == 3:\n",
    "            residual_clone = einops.repeat(residual_clone, 'b p d -> b p h d', h=1)\n",
    "        residual_clone = residual_clone[:, token_idx, :, :]  # [b h d]\n",
    "        logits = model.unembed(model.ln_final(residual_clone))  # [b h v]\n",
    "        probs = F.softmax(logits, dim=-1) # [b h v]\n",
    "        probs = einops.reduce(probs, 'b h v -> h v', 'mean')\n",
    "        target_probs = torch.zeros_like(logit_lens[loc[0]:loc[1], layer_idx])\n",
    "        for token_id in target_id:\n",
    "            target_probs += probs[:, token_id.item()].cpu()  # [h]\n",
    "        logit_lens[loc[0]:loc[1], layer_idx] = target_probs\n",
    "        \n",
    "        topk_probs, topk_indices = torch.topk(probs, k, dim=-1)  # [h k]\n",
    "        for h_idx in range(topk_indices.shape[0]):\n",
    "            topk_str_tokens = model.to_str_tokens(topk_indices[h_idx])\n",
    "            topk_str_tokens = [f\"{t[:10]} {topk_probs[h_idx,i].item():.2f}\" for i,t in enumerate(topk_str_tokens)]\n",
    "            annotations.append(\n",
    "                dict(\n",
    "                    text='<br>'.join(topk_str_tokens),\n",
    "                    x=layer_idx,\n",
    "                    y=h_idx + loc[0],\n",
    "                    showarrow=False,\n",
    "                    font=dict(color='white',size=10)\n",
    "                )\n",
    "            )\n",
    "        return residual\n",
    "\n",
    "    names_filter = lambda x: any([name in x for name in residual_loc.keys()])\n",
    "    out = model.run_with_hooks(prompt_id, return_type=None, fwd_hooks=[\n",
    "        (names_filter, logit_lens_hook),\n",
    "    ])\n",
    "    fig = go.Figure(data=go.Heatmap(z=logit_lens, colorscale='Viridis_r'))\n",
    "    x_labels = [f\"Layer {i}\" for i in range(model.cfg.n_layers)]\n",
    "    y_labels = [f\"Head {i}\" for i in range(model.cfg.n_heads)] + ['Resid Pre', 'Resid Mid', 'Resid Post', 'All Heads Out', 'Mlp Out']\n",
    "    fig.update_layout(\n",
    "        title=f\"{prompt[0]}->[{', '.join(target_token)}]\",\n",
    "        xaxis_title='Layer',\n",
    "        yaxis_title='Head',\n",
    "        xaxis=dict(ticktext=x_labels, tickvals=list(range(model.cfg.n_layers))),\n",
    "        yaxis=dict(ticktext=y_labels, tickvals=list(range(model.cfg.n_heads + 5))),\n",
    "        height=1600,\n",
    "        width=1600,\n",
    "        annotations=annotations,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def get_head_pattern(model: HookedTransformer, prompt, layer_idx, head_idx):\n",
    "    out, cache = model.run_with_cache(prompt, names_filter=utils.get_act_name('pattern', layer_idx), device='cpu')\n",
    "    cache = cache[utils.get_act_name('pattern', layer_idx)]  # [b h pq pk]\n",
    "    pattern = einops.reduce(cache[:, head_idx, :, :], 'b pq pk -> pq pk', 'mean')\n",
    "    fig = go.Figure(data=go.Heatmap(z=pattern.T))\n",
    "    str_tokens = model.to_str_tokens(prompt)\n",
    "    fig.update_layout(\n",
    "        title=f\"({layer_idx}.{head_idx})\", \n",
    "        xaxis_title='query', \n",
    "        yaxis_title='key',\n",
    "        xaxis=dict(ticktext=str_tokens, tickvals=list(range(model.cfg.n_layers))),\n",
    "        yaxis=dict(ticktext=str_tokens, tickvals=list(range(model.cfg.n_heads))),\n",
    "        height=1000, \n",
    "        width=1000\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_cache = get_random_head_cache()\n",
    "zero_cache = torch.zeros_like(random_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exp_head_ablation(model: HookedTransformer, test_prompts, test_answers):\n",
    "    def get_correct_logits_and_probs(model, test_prompts, test_answers):\n",
    "        out_logits = model(test_prompts)[:, -1, :]\n",
    "        correct_idxs = model.to_tokens(test_answers, prepend_bos=False)\n",
    "        out_probs = F.softmax(out_logits, dim=-1)\n",
    "        correct_logits = torch.diag(out_logits[torch.arange(len(out_logits)), correct_idxs])\n",
    "        correct_probs = torch.diag(out_probs[torch.arange(len(out_probs)), correct_idxs])\n",
    "        return correct_logits.mean(), correct_probs.mean()\n",
    "\n",
    "    model.reset_hooks()\n",
    "    org_logits, org_probs = get_correct_logits_and_probs(model, test_prompts, test_answers)\n",
    "    all_lc_matrixs = []\n",
    "    all_pc_matrixs = []\n",
    "    cache_list = [zero_cache, random_cache]\n",
    "    cache_names = [\"zero_cache\", \"random_cache\"]\n",
    "    for cache, name in zip(cache_list, cache_names):\n",
    "        print(f\"running {name}\")\n",
    "        change_in_correct_probs = torch.empty((model.cfg.n_heads, model.cfg.n_layers))\n",
    "        change_in_correct_logits = torch.empty((model.cfg.n_heads, model.cfg.n_layers))\n",
    "        for layer_idx in tqdm(range(model.cfg.n_layers)):\n",
    "            for head_idx in range(model.cfg.n_heads):\n",
    "                model.reset_hooks()\n",
    "                model = single_head_ablation(model, layer_idx, head_idx, cache)\n",
    "                logits, probs = get_correct_logits_and_probs(model, test_prompts, test_answers)\n",
    "                change_in_correct_probs[head_idx, layer_idx] = probs - org_probs\n",
    "                change_in_correct_logits[head_idx, layer_idx] = logits - org_logits\n",
    "        all_lc_matrixs.append(change_in_correct_logits)\n",
    "        all_pc_matrixs.append(change_in_correct_probs)\n",
    "\n",
    "    def _plot_heatmap(matrixs, title):\n",
    "        fig = make_subplots(rows=1, cols=3, subplot_titles=cache_names)\n",
    "        for i, matrix in enumerate(matrixs):\n",
    "            fig.add_trace(go.Heatmap(z=matrix, colorscale='Viridis_r', showscale=(i == 0)), row=1, col=i + 1)\n",
    "        fig.update_layout(title=title, xaxis_title='Layer', yaxis_title='Head', height=400, width=1200)\n",
    "        fig.show()\n",
    "\n",
    "    _plot_heatmap(all_pc_matrixs, \"Change in correct probs\")\n",
    "    _plot_heatmap(all_lc_matrixs, \"Change in correct logits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_prompts = [q for (q,a) in test_dst[:10]]\n",
    "test_answers = [a for (q,a) in test_dst[:10]]\n",
    "exp_head_ablation(model, test_prompts, test_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_dataset = [q for (q,a) in test_dst if a == \" Positive\"]\n",
    "neg_dataset = [q for (q,a) in test_dst if a == \" Negative\"]\n",
    "pos_tokens = [\" Positive\", \"Positive\", \"positive\", \" positive\"]\n",
    "neg_tokens = [\" Negative\", \"Negative\", \"negative\", \" negative\"]\n",
    "full_logit_lens(model, pos_dataset[:100], pos_tokens, k=5)\n",
    "full_logit_lens(model, neg_dataset[:100], neg_tokens, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
