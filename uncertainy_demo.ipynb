{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3ad78fe71283d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inplementation of Internal State-based Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278c24516568ba0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 数据集预处理：将不同格式的数据集处理成input+gt的格式，方便判断模型的correctness，这一部分的采用固定不可调整的prompt，即Context: Question: Options: Answer:格式\n",
    "2. 生成回复，为每个模型确定一个prompt，一个max_new_tokens数，然后生成回复\n",
    "3. 计算回复部分的correctness指标，判断模型的回复是否正确\n",
    "4. 计算uncertainty指标，包括PE, LN-PE, SAR, Ours\n",
    "5. 计算AUROC，绘制AUROC/Correctness-Threshold曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T06:02:47.661199Z",
     "start_time": "2024-04-01T06:02:46.810601Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩余内存: 597.0 G\n",
      "当前主机名是:SH-IDC1-10-140-0-183\n",
      "SH-IDC1-10-140-0-183      Mon Apr  1 14:02:47 2024  525.60.13\n",
      "[0] NVIDIA A100-SXM4-80GB | 57°C, 100 % | 67905 / 81920 MB | gaopeng(67902M)\n",
      "[1] NVIDIA A100-SXM4-80GB | 69°C, 100 % | 67151 / 81920 MB | gaopeng(67148M)\n",
      "[2] NVIDIA A100-SXM4-80GB | 29°C,   0 % |     0 / 81920 MB |\n",
      "[3] NVIDIA A100-SXM4-80GB | 27°C,   0 % |     0 / 81920 MB |\n",
      "[4] NVIDIA A100-SXM4-80GB | 27°C,   0 % | 17329 / 81920 MB | guoyiqiu(17326M)\n",
      "[5] NVIDIA A100-SXM4-80GB | 29°C,   0 % |     0 / 81920 MB |\n",
      "[6] NVIDIA A100-SXM4-80GB | 45°C,  61 % | 68729 / 81920 MB | gaopeng(68726M)\n",
      "[7] NVIDIA A100-SXM4-80GB | 39°C,  61 % | 68729 / 81920 MB | gaopeng(68726M)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "os.environ['HF_DATASETS_OFFLINE'] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "import transformer_lens\n",
    "import datasets\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, Features, Array2D, Array3D\n",
    "from typing import List, Tuple, Union\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as st_util\n",
    "from transformers import pipeline\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "import math\n",
    "\n",
    "datasets.disable_caching()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "def print_sys_info():\n",
    "    import psutil\n",
    "    import socket\n",
    "    import gpustat\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(\"剩余内存: {} G\".format(memory.available / 1024 / 1024 // 1024))\n",
    "    host_name = socket.gethostname()\n",
    "    print(f\"当前主机名是:{host_name}\")\n",
    "    gpustat.print_gpustat()\n",
    "\n",
    "print_sys_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a702b3ebfcfa6944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T05:31:46.774568Z",
     "start_time": "2024-04-01T05:31:30.327856Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a1e2f5e220407a8f9d7ccccc298ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/guoyiqiu/miniconda3/envs/mi/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# Model Config\n",
    "model_name = \"vicuna-7b-v1.1\"\n",
    "hooked_transformer_name = \"llama-7b-hf\"\n",
    "hf_model_path = os.path.join(os.environ[\"my_models_dir\"], model_name)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(hf_model_path)\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(hooked_transformer_name, dtype='bfloat16', hf_model=hf_model, tokenizer=hf_tokenizer, default_padding_side='left')\n",
    "\n",
    "# Aux Models\n",
    "se_bert_name = \"microsoft/deberta-large-mnli\"\n",
    "nli_pipe = pipeline(\"text-classification\", model=se_bert_name, device=0)\n",
    "\n",
    "# sar_bert_name = 'cross-encoder/stsb-roberta-large'\n",
    "# # sar_bert_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# sar_bert = SentenceTransformer(sar_bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fd8e1d7498093f98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T06:10:29.209042Z",
     "start_time": "2024-04-01T06:10:29.200636Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Timer:\n",
    "    def __enter__(self):\n",
    "        self.ts = time()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        self.te = time()\n",
    "        self.t = self.te - self.ts\n",
    "        \n",
    "def _get_answer_prob(inp, out, prob):\n",
    "    num_input_tokens = len(model.to_str_tokens(inp))\n",
    "    output_tokens = model.to_tokens(out, move_to_device=False)[0].tolist()\n",
    "    if len(output_tokens) == num_input_tokens:\n",
    "        return []\n",
    "    answer_tokens = output_tokens[num_input_tokens:]\n",
    "    answer_prob = prob[num_input_tokens - 1:-1, :]\n",
    "    answer_prob = answer_prob[range(len(answer_tokens)), answer_tokens]\n",
    "    answer_prob = answer_prob.tolist()\n",
    "    return answer_prob\n",
    "        \n",
    "def get_sampled_answer_prob(example):\n",
    "    batch_answer_prob = []\n",
    "    washed_sampled_output = example['washed_sampled_output']\n",
    "    washed_sampled_output_unique = list(set(washed_sampled_output))\n",
    "    batch_prob = F.softmax(model(washed_sampled_output_unique, padding_side='right'), dim=-1)  # logits: (bsz pos vocab)\n",
    "\n",
    "    for i in range(len(example['washed_sampled_output'])):\n",
    "        inp = example['input']\n",
    "        out = example['washed_sampled_output'][i]\n",
    "        prob = batch_prob[washed_sampled_output_unique.index(out)]\n",
    "        answer_prob = _get_answer_prob(inp, out, prob)\n",
    "        batch_answer_prob.append(answer_prob)\n",
    "\n",
    "    example['sampled_answer_prob'] = batch_answer_prob\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_se(example, nli_pipe, eps=1e-9):\n",
    "    # Sample Answers\n",
    "    washed_sampled_answer = example['washed_sampled_answer']\n",
    "    if not example.get('sampled_answer_prob'):\n",
    "        example = get_sampled_answer_prob(example)\n",
    "    with Timer() as timer:\n",
    "        # Bidirectional Entailment Clustering\n",
    "        meanings = [[washed_sampled_answer[0]]]\n",
    "        seqs = washed_sampled_answer[1:]\n",
    "        for s in seqs:\n",
    "            in_existing_meaning = False\n",
    "            for c in meanings:\n",
    "                s_c = c[0]\n",
    "                tmp = \"[CLS] {s1} [SEP] {s2} [CLS]\"\n",
    "                res = nli_pipe([tmp.format(s1=s, s2=s_c), tmp.format(s1=s_c, s2=s)])\n",
    "                if res[0]['label'] == 'ENTAILMENT' and res[1]['label'] == 'ENTAILMENT':\n",
    "                    c.append(s)\n",
    "                    in_existing_meaning = True\n",
    "                    break\n",
    "            if not in_existing_meaning:\n",
    "                meanings.append([s])\n",
    "        # Calculate Semantic Entropy\n",
    "        pcs = []\n",
    "        for c in meanings:\n",
    "            pc = eps\n",
    "            for s in c:\n",
    "                idx = example['washed_sampled_answer'].index(s)\n",
    "                answer_prob = example['sampled_answer_prob'][idx]\n",
    "                ps = np.prod(answer_prob)\n",
    "                pc += ps\n",
    "            pcs.append(pc)\n",
    "        example['u_score_se'] = -np.sum(np.log(pcs) * pcs)\n",
    "    example['time_se'] = timer.t\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df0eeae0abf3ba21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T06:09:58.307047Z",
     "start_time": "2024-04-01T06:09:52.924561Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b47cd0a0cf42fe919a4db4d053f361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_dst = Dataset.load_from_disk(\"/mnt/petrelfs/guoyiqiu/coding/trainable_uncertainty/cached_results/short/allenai_sciq_validation_1000_vicuna-7b-v1.1\")\n",
    "# test_dst = test_dst.filter(lambda x: 'Esters can be formed by heating carboxylic acids and alcohols in the presence of' in x['question'])\n",
    "test_dst = test_dst.select(range(100,150))\n",
    "test_dst = test_dst.map(get_sampled_answer_prob, new_fingerprint=str(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77486b0d199c08ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T06:11:07.525529Z",
     "start_time": "2024-04-01T06:10:30.685600Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5729009fedb4dd9a6bea6ee706defec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'distractor3', 'distractor1', 'distractor2', 'correct_answer', 'support', 'input', 'dst_template', 'options', 'gt', 'answer', 'washed_answer', 'output', 'washed_output', 'sampled_answer', 'washed_sampled_answer', 'sampled_output', 'washed_sampled_output', 'sampled_answer_prob', 'u_score_se', 'time_se'],\n",
       "    num_rows: 50\n",
       "})"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dst.map(partial(get_uncertainty_score_se,nli_pipe=nli_pipe), new_fingerprint=str(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "60fd39cbfe215026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T06:45:59.154928Z",
     "start_time": "2024-04-01T06:45:59.109030Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
       "        num_rows: 10178\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['question', 'answer', 'options', 'meta_info', 'answer_idx', 'metamap_phrases'],\n",
       "        num_rows: 1273\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst = datasets.load_dataset(\"/mnt/petrelfs/guoyiqiu/coding/huggingface/datasets/GBaker___med_qa-usmle-4-options/default/0.0.0/0fb93dd23a7339b6dcd27e241cb9b5eca62d4d18\")\n",
    "dst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edd9cca2e9bbfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T14:41:16.342685Z",
     "start_time": "2024-03-24T14:41:16.293234Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our Method OLD\n",
    "def compute_certainty_vector_mean(train_dst, layers, act_name, batch_size=8):\n",
    "    def get_paired_dst_sciq(train_dst):\n",
    "        tmp_pos = \"Question:{q} Options:{o} The correct answer is:\"\n",
    "        tmp_neg = \"Question:{q} Options:{o} The incorrect answer is:\"\n",
    "    \n",
    "        # sciq_train_dst = sciq_train_dst.filter(lambda x: x['rougel'] > 0.5)\n",
    "    \n",
    "        def get_pos_example(example):\n",
    "            example['input'] = tmp_pos.format(q=example['question'], o=\", \".join(example['options']))\n",
    "            example['washed_output'] = f\"{example['input']}{example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            example['input'] = tmp_neg.format(q=example['question'], o=\", \".join(example['options']))\n",
    "            wrong_options = [opt for opt in example['options'] if opt != example['gt']]\n",
    "            if wrong_options:\n",
    "                random.seed(42 + idx)\n",
    "                wrong_answer = random.choice(wrong_options)\n",
    "            else:\n",
    "                wrong_answer = \"wrong answer\"\n",
    "            example['washed_output'] = f\"{example['input']}{wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    \n",
    "    def get_paired_dst_coqa(train_dst):\n",
    "        def get_pos_example(example):\n",
    "            example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            wrong_options = [opt for opt in example['answers']['input_text'] if opt != example['gt']]\n",
    "            if wrong_options:\n",
    "                random.seed(42 + idx)\n",
    "                wrong_answer = random.choice(wrong_options)\n",
    "            else:\n",
    "                wrong_answer = \"wrong answer\"\n",
    "            example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    \n",
    "    def get_paired_dst_triviaqa(train_dst):\n",
    "        def get_pos_example(example):\n",
    "            example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            next_idx = idx + 1 if idx + 1 < len(train_dst) else 0\n",
    "            wrong_answer = train_dst[next_idx]['gt']\n",
    "            example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    \n",
    "    def get_paired_dst_medmcqa(train_dst):\n",
    "        def get_pos_example(example):\n",
    "            example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            wrong_options = [opt for opt in example['options'] if opt != example['gt']]\n",
    "            if wrong_options:\n",
    "                random.seed(42 + idx)\n",
    "                wrong_answer = random.choice(wrong_options)\n",
    "            else:\n",
    "                wrong_answer = \"wrong answer\"\n",
    "            example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    func_map = {\n",
    "        'allenai/sciq': get_paired_dst_sciq,\n",
    "        'stanfordnlp/coqa': get_paired_dst_coqa,\n",
    "        'lucadiliello/triviaqa': get_paired_dst_triviaqa,\n",
    "        'openlifescienceai/medmcqa': get_paired_dst_medmcqa,\n",
    "        'GBaker/MedQA-USMLE-4-options': get_paired_dst_medmcqa\n",
    "    }\n",
    "    func = func_map[dst_name]\n",
    "    dst_pos, dst_neg = func(train_dst)\n",
    "    \n",
    "    data_pos = dst_pos['washed_output']\n",
    "    data_neg = dst_neg['washed_output']\n",
    "    data_size = len(data_pos)\n",
    "    full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "    v_c = torch.zeros((len(layers), 1, model.cfg.d_model)).cuda()\n",
    "\n",
    "    for i in tqdm(range(0, data_size, batch_size)):\n",
    "        batch_pos = data_pos[i:i + batch_size]\n",
    "        batch_neg = data_neg[i:i + batch_size]\n",
    "\n",
    "        _, cache_pos = model.run_with_cache(batch_pos, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "        _, cache_neg = model.run_with_cache(batch_neg, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "\n",
    "        cache_pos = einops.rearrange([cache_pos[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "        cache_neg = einops.rearrange([cache_neg[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "\n",
    "        cache_pos = cache_pos[:, :, [-1], :]\n",
    "        cache_neg = cache_neg[:, :, [-1], :]\n",
    "\n",
    "        v_c += (cache_pos.sum(dim=0) - cache_neg.sum(dim=0))\n",
    "\n",
    "    v_c /= data_size\n",
    "\n",
    "    v_c = v_c.cpu().float()\n",
    "    v_c = F.normalize(v_c, p=2, dim=-1)\n",
    "    return v_c\n",
    "\n",
    "\n",
    "# clean_exp exp\n",
    "def clean_exp(dst, v_c, layers, act_name):\n",
    "    fig = go.Figure()\n",
    "    c_scores = []\n",
    "    w_scores = []\n",
    "    labels = []\n",
    "    u_scores = []\n",
    "    u_scores_z = []\n",
    "    all_pe_u_scores = []\n",
    "    all_ln_pe_u_scores = []\n",
    "\n",
    "    def batch_get_result(examples):\n",
    "        all_outputs = []\n",
    "        all_num_answer_tokens = []\n",
    "        all_num_input_tokens = list(map(len, model.to_str_tokens(examples['input'])))\n",
    "        bsz = len(examples['input'])\n",
    "\n",
    "        for i in range(bsz):\n",
    "            example = {k: examples[k][i] for k in examples.keys()}\n",
    "            if example.get(\"options\"):\n",
    "                wrong_options = [opt for opt in example['options']]\n",
    "                for opt in wrong_options:\n",
    "                    if opt == example['gt']:\n",
    "                        wrong_options.remove(opt)\n",
    "                        break\n",
    "            elif example.get(\"answers\"):\n",
    "                wrong_options = [opt for opt in example['answers']['input_text']]\n",
    "                for opt in wrong_options:\n",
    "                    if opt == example['gt']:\n",
    "                        wrong_options.remove(opt)\n",
    "                        break\n",
    "                wrong_options = wrong_options[:3]\n",
    "            else:\n",
    "                wrong_options = ['wrong answer', 'bad answer', 'incorrect answer']\n",
    "            correct_output = example['input'] + example['gt']\n",
    "            wrong_outputs = [example['input'] + opt for opt in wrong_options]\n",
    "            all_outputs.extend([correct_output] + wrong_outputs)\n",
    "            num_answer_tokens = list(map(len, model.to_str_tokens([example['gt']] + wrong_options)))\n",
    "            all_num_answer_tokens.append(num_answer_tokens)\n",
    "\n",
    "        full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "\n",
    "        batch_logits, batch_cache = model.run_with_cache(all_outputs, names_filter=lambda x: x in full_act_names,\n",
    "                                                         device='cpu',\n",
    "                                                         padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "        batch_cache = einops.rearrange([batch_cache[name] for name in full_act_names],\n",
    "                                       'l b p d -> b l p d').float().cpu()\n",
    "        batch_cache = einops.rearrange(batch_cache, '(b o) l p d -> b o l p d', o=4)\n",
    "        batch_cache = batch_cache[:, :, :, [-1], :]\n",
    "\n",
    "        batch_logits = batch_logits.cpu().float()\n",
    "        batch_logits = einops.rearrange(batch_logits, '(b o) p v -> b o p v', o=4)\n",
    "\n",
    "        for i, lg_4 in enumerate(batch_logits):\n",
    "            num_answer_tokens = all_num_answer_tokens[i]\n",
    "            num_input_tokens = all_num_input_tokens[i]\n",
    "            for j, lg in enumerate(lg_4):\n",
    "                output = all_outputs[i * 4 + j]\n",
    "                answer_lg = lg[-num_answer_tokens[j] - 1:-1]\n",
    "                answer_prob = F.softmax(answer_lg, dim=-1)\n",
    "                answer_target_prob = answer_prob.max(dim=-1).values\n",
    "                pe = -torch.log(answer_target_prob).sum().item()\n",
    "                # print(f\"pe:{pe}\")\n",
    "                ln_pe = -torch.log(answer_target_prob).mean().item()\n",
    "                # print(f\"ln_pe:{ln_pe}\")\n",
    "                all_pe_u_scores.append(pe)\n",
    "                all_ln_pe_u_scores.append(ln_pe)\n",
    "\n",
    "        batch_in_vivo_auroc = []\n",
    "        for i in range(bsz):\n",
    "            cache = batch_cache[i]\n",
    "            u_score = einsum('b l p d, l p d -> b', cache, v_c)\n",
    "            u_score_z = (u_score - u_score.mean()) / u_score.std()\n",
    "\n",
    "            u_score = u_score.tolist()\n",
    "            u_score_z = u_score_z.tolist()\n",
    "\n",
    "            in_vivo_auroc = roc_auc_score([1, 0, 0, 0], u_score)\n",
    "            batch_in_vivo_auroc.append(in_vivo_auroc)\n",
    "            # if u_score[0] > max(u_score[1:]):\n",
    "            #     batch_in_vivo_auroc.append(1)\n",
    "            # else:\n",
    "            #     batch_in_vivo_auroc.append(0)\n",
    "\n",
    "            c_scores.append(u_score_z[0])\n",
    "            w_scores.extend(u_score_z[1:])\n",
    "            labels.extend([1, 0, 0, 0])\n",
    "\n",
    "            # assert len(u_score) == 4, f\"{len(u_score)} {example['options']}\"\n",
    "            u_scores.extend(u_score)\n",
    "            u_scores_z.extend(u_score_z)\n",
    "\n",
    "        examples['in_vivo_auroc'] = batch_in_vivo_auroc\n",
    "        return examples\n",
    "\n",
    "    new_dst = dst.map(batch_get_result, new_fingerprint=str(time()), batched=True, batch_size=4)\n",
    "\n",
    "    in_vivo_auroc = sum(new_dst['in_vivo_auroc']) / len(new_dst['in_vivo_auroc'])\n",
    "    flag = in_vivo_auroc > 0.5\n",
    "    in_vivo_auroc = in_vivo_auroc if flag else 1 - in_vivo_auroc\n",
    "    print(f\"in-vivo u_score auroc: {in_vivo_auroc}\")\n",
    "\n",
    "    in_vitro_auroc = roc_auc_score(labels, u_scores)\n",
    "    in_vitro_auroc = in_vitro_auroc if flag else 1 - in_vitro_auroc\n",
    "    print(f\"in-vitro u_score auroc: {in_vitro_auroc}\")\n",
    "\n",
    "    in_vitro_auroc_z = roc_auc_score(labels, u_scores_z)\n",
    "    in_vitro_auroc_z = in_vitro_auroc_z if flag else 1 - in_vitro_auroc_z\n",
    "    print(f\"in-vitro u_score_z auroc: {in_vitro_auroc_z}\")\n",
    "\n",
    "    in_vitro_pe_auroc = roc_auc_score(labels, all_pe_u_scores)\n",
    "    print(f\"in-vitro pe auroc: {in_vitro_pe_auroc}\")\n",
    "\n",
    "    in_vitro_ln_pe_auroc = roc_auc_score(labels, all_ln_pe_u_scores)\n",
    "    print(f\"in-vitro ln_pe auroc: {in_vitro_ln_pe_auroc}\")\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=c_scores, name='Correct', opacity=0.5, nbinsx=100))\n",
    "    fig.add_trace(go.Histogram(x=w_scores, name='Wrong', opacity=0.5, nbinsx=100))\n",
    "    fig.update_layout(barmode='overlay')\n",
    "    fig.show()\n",
    "\n",
    "'''\n",
    "Compared Baseline Methods:\n",
    "PE(Predictive Entropy),\n",
    "LN-PE(Length-normalised Predictive Entropy),\n",
    "SAR(Shifting Attention to more Relevant),\n",
    "LS(Lexical Similarity),\n",
    "SE(Semantic Entropy),\n",
    "SR(Self-Report)\n",
    "Ours(Activation Based)\n",
    "'''\n",
    "\n",
    "# Evaluation: AUROC with Correctness Metric\n",
    "def get_auroc(val_dst, u_metric, c_metric, c_th):\n",
    "    c_metrics = val_dst[c_metric]\n",
    "    label = [1 if c > c_th else 0 for c in c_metrics]\n",
    "    u_score = val_dst[u_metric]\n",
    "    auroc = roc_auc_score(label, u_score)\n",
    "    auroc = auroc if auroc > 0.5 else 1 - auroc\n",
    "    return auroc\n",
    "\n",
    "def plot_th_curve(test_dst, u_metrics, c_metric, nbins=10):\n",
    "    fig = go.Figure()\n",
    "    th_range = [i / nbins for i in range(1, nbins)]\n",
    "    accs = []\n",
    "    c_metrics = test_dst[c_metric]\n",
    "\n",
    "    for th in th_range:\n",
    "        acc = 0\n",
    "        for c in c_metrics:\n",
    "            if c > th:\n",
    "                acc += 1\n",
    "        acc = acc / len(c_metrics)\n",
    "        accs.append(acc)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=th_range, y=accs, mode='lines+markers+text', name=f\"acc\", text=[f\"{a:.4f}\" for a in accs], textposition=\"top center\"))\n",
    "\n",
    "    for u_metric in u_metrics:\n",
    "        aurocs = []\n",
    "        for th in th_range:\n",
    "            aurocs.append(get_auroc(test_dst, u_metric, c_metric, th))\n",
    "        fig.add_trace(go.Scatter(x=th_range, y=aurocs, mode='lines+markers+text', name=f\"{u_metric}\", text=[f\"{a:.4f}\" for a in aurocs], textposition=\"top center\"))\n",
    "    fig.update_layout(title=f\"AUROC/{c_metric}-Threshold Curve\", xaxis_title=f\"{c_metric}-Threshold\", yaxis_title=\"AUROC\", width=2000, height=1000)\n",
    "    fig.write_image(f\"eval_results/{dst_name}_{model_name}_{c_metric}_th_curve.png\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a9129651081dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-24T14:31:25.812191Z",
     "start_time": "2024-03-24T14:31:25.124814Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u_metrics = [k for k in test_dst[0].keys() if k.startswith(\"u_score\") and not k.endswith(\"all\")]\n",
    "fig = plot_th_curve(test_dst, u_metrics, 'rougel')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db69225b79de49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T15:20:35.189620Z",
     "start_time": "2024-03-21T15:20:35.065244Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = test_dst.filter(lambda x: x['rougel'] < 0.1)[1]\n",
    "example = test_dst[2]\n",
    "print(f\"gt:{example['gt']}\")\n",
    "print(f\"options:{example['options']}\")\n",
    "\n",
    "str_tokens = model.to_str_tokens(f\":{example['washed_answer']}\", prepend_bos=False)[1:]\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Token Level\", \"Sentence Level\"), row_heights=[0.5, 0.5])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(str_tokens))), y=example['u_score_pe_all'], mode='lines+markers'), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Token', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=1, col=1)\n",
    "\n",
    "sentence_u_score_pe_all = []\n",
    "indices = [0]+[i for i, x in enumerate(str_tokens) if x == '.']+[-1]\n",
    "spans = [(indices[i], indices[i+1]) for i in range(len(indices)-1)]\n",
    "print(len(indices))\n",
    "for span in spans:\n",
    "    sentence_score = sum(example['u_score_pe_all'][span[0]:span[1]]) / (span[1] - span[0])\n",
    "    sentence_u_score_pe_all.extend([sentence_score] * (span[1] - span[0]))\n",
    "sentence_u_score_pe_all.append(sentence_u_score_pe_all[-1])\n",
    "# print(str_tokens)\n",
    "for i, sentence in enumerate(example['washed_answer'].split(\".\")):\n",
    "    print(i+1,sentence.replace(\"\\n\",' ').strip())\n",
    "# print(len(example['u_score_pe_all']))\n",
    "# print(sentence_u_score_pe_all)\n",
    "# print(len(sentence_u_score_pe_all))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(str_tokens))), y=sentence_u_score_pe_all, mode='lines+markers'), row=2, col=1)\n",
    "fig.update_xaxes(title_text='Sentence', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=1000, width=2500, margin=dict(l=0, r=0, b=50, t=50), title_text=example['washed_answer'])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
