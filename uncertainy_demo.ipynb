{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3ad78fe71283d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inplementation of Internal State-based Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278c24516568ba0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 数据集预处理：将不同格式的数据集处理成input+gt的格式，方便判断模型的correctness，这一部分的采用固定不可调整的prompt，即Context: Question: Options: Answer:格式\n",
    "2. 生成回复，为每个模型确定一个prompt，一个max_new_tokens数，然后生成回复\n",
    "3. 计算回复部分的correctness指标，判断模型的回复是否正确\n",
    "4. 计算uncertainty指标，包括PE, LN-PE, SAR, Ours\n",
    "5. 计算AUROC，绘制AUROC/Correctness-Threshold曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T01:00:59.283424Z",
     "start_time": "2024-04-09T01:00:40.264892Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "datasets.disable_caching()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# Eval Result Config\n",
    "model_names = [\n",
    "    \"vicuna-7b-v1.1\",\n",
    "    \"vicuna-13b-v1.1\",\n",
    "    \"vicuna-33b-v1.3\",\n",
    "]\n",
    "\n",
    "dst_names = [\n",
    "    \"sciq\",\n",
    "    \"coqa\",\n",
    "    \"triviaqa\",\n",
    "    \"medmcqa\",\n",
    "    \"MedQA-USMLE-4-options\",\n",
    "]\n",
    "\n",
    "c_metrics = [\n",
    "    'rougel',\n",
    "    'sentsim',\n",
    "    'include'\n",
    "]\n",
    "\n",
    "dst_types = [\n",
    "    \"short\",\n",
    "    \"long\",\n",
    "]\n",
    "\n",
    "acc_map = {\n",
    "    \"vicuna-7b-v1.1\": {\n",
    "        \"sciq\": 0.60,\n",
    "        \"coqa\": 0.8,\n",
    "        \"triviaqa\": 0.55,\n",
    "        \"medmcqa\": 0.30,\n",
    "        \"MedQA-USMLE-4-options\": 0.30\n",
    "    },\n",
    "    \"vicuna-13b-v1.1\": {\n",
    "        \"sciq\": 0.0,\n",
    "        \"coqa\": 0.0,\n",
    "        \"triviaqa\": 0.0,\n",
    "        \"medmcqa\": 0.0,\n",
    "        \"MedQA-USMLE-4-options\": 0.0\n",
    "    },\n",
    "    \"vicuna-33b-v1.3\": {\n",
    "        \"sciq\": 0.0,\n",
    "        \"coqa\": 0.0,\n",
    "        \"triviaqa\": 0.0,\n",
    "        \"medmcqa\": 0.0,\n",
    "        \"MedQA-USMLE-4-options\": 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "model_names_alias = {\n",
    "    \"vicuna-7b-v1.1\": \"Vicuna-7B\",\n",
    "    \"vicuna-13b-v1.1\": \"Vicuna-13B\",\n",
    "    \"vicuna-33b-v1.3\": \"Vicuna-33B\"\n",
    "}\n",
    "\n",
    "dst_names_alias = {\n",
    "    \"sciq\": \"SciQ\",\n",
    "    \"coqa\": \"CoQA\",\n",
    "    \"triviaqa\": \"TriviaQA\",\n",
    "    \"medmcqa\": \"MedMCQA\",\n",
    "    \"MedQA-USMLE-4-options\": \"MedQA\"\n",
    "}\n",
    "\n",
    "u_metric_alias = {\n",
    "    \"u_score_pe\": \"PE\",\n",
    "    \"u_score_ln_pe\": \"LN-PE\",\n",
    "    \"u_score_token_sar\": \"TokenSAR\",\n",
    "    \"u_score_sent_sar\": \"SentSAR\",\n",
    "    \"u_score_sar\": \"SAR\",\n",
    "    \"u_score_ls\": \"LS\",\n",
    "    \"u_score_se\": \"SE\",\n",
    "    \"u_score_ours_mean_soft_rougel\": \"Ours(MSRL)\",\n",
    "    \"u_score_ours_last_soft_rougel\": \"Ours(LSRL)\",\n",
    "    \"u_score_ours_mean_soft_include\": \"Ours(MSIN)\",\n",
    "    \"u_score_ours_last_soft_include\": \"Ours(LSIN)\",\n",
    "    \"u_score_ours_mean_soft_sentsim\": \"Ours(MSSI)\",\n",
    "    \"u_score_ours_last_soft_sentsim\": \"Ours(LSSI)\"\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "def get_cached_result_path(model_name, dst_name, dst_type, dst_split):\n",
    "    return f\"cached_results/{model_name}/{dst_type}/{dst_name}_{dst_split}\"\n",
    "\n",
    "\n",
    "def get_eval_main_result_path(model_name, dst_name, dst_type):\n",
    "    return f\"eval_results/{model_name}/{dst_name}_{dst_type}\"\n",
    "\n",
    "\n",
    "def get_eval_cross_result_path(model_name, train_dst_name, train_dst_type, test_dst_name, test_dst_type, c_metric):\n",
    "    return f\"cross_eval_results/{model_name}/{c_metric}/v_c_{train_dst_name}_{train_dst_type}_mean_soft_best.pth/{test_dst_name}_{test_dst_type}\"\n",
    "\n",
    "\n",
    "def get_c_th_by_acc(test_dst, c_metric, acc):\n",
    "    sorted_c_scores = sorted(list(test_dst[c_metric]), reverse=True)\n",
    "    c_th = sorted_c_scores[int(len(sorted_c_scores) * acc)]\n",
    "    return c_th\n",
    "\n",
    "\n",
    "def get_acc_by_c_th(test_dst, c_metric, c_th):\n",
    "    return sum([1 if s > c_th else 0 for s in test_dst[c_metric]]) / len(test_dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c273cf4cd288e17d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-09T01:43:09.338750Z",
     "start_time": "2024-04-09T01:43:00.097250Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a60384bb5df64aa88e07e7d4ced2f6e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: sciq_short\n",
      "Length: 2000\n",
      "Empty Answer: 7\n",
      "Samples:\n",
      "question: What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
      "washed_answer: bacteria\n",
      "\n",
      "question: What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
      "washed_answer: coriolis effect\n",
      "\n",
      "question: Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?\n",
      "washed_answer: Endothermic\n",
      "\n",
      "[2744, 29876, 1474] Annually\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c92a0fcc68c043efa6b54a6d719550f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: sciq_long\n",
      "Length: 2000\n",
      "Empty Answer: 1\n",
      "Samples:\n",
      "question: What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
      "washed_answer: The type of organism commonly used in the preparation of foods such as cheese and yogurt is:\n",
      "\n",
      "question: What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
      "washed_answer: The phenomenon that makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere is the Coriolis effect\n",
      "\n",
      "question: Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always what?\n",
      "washed_answer: Changes from a less-ordered state to a more-ordered state (such as a liquid to a solid) are always endothermic\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f49f19e5b874092893e6670cbe0eafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: coqa_short\n",
      "Length: 2000\n",
      "Empty Answer: 1\n",
      "Samples:\n",
      "question: When was the Vat formally opened?\n",
      "washed_answer: The Vatican Library was formally established in 1475\n",
      "\n",
      "question: Where was the Auction held?\n",
      "washed_answer: The auction was held at the Hard Rock Cafe in New York's Times Square\n",
      "\n",
      "question: What did Venters call Lassiter?\n",
      "washed_answer: Venters called Lassiter a gun-man\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef565af43c542b297198a7d5bd58941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: coqa_long\n",
      "Length: 2000\n",
      "Empty Answer: 0\n",
      "Samples:\n",
      "question: When was the Vat formally opened?\n",
      "washed_answer: The Vatican Apostolic Library (Vatican Library or Vat) was formally established in 1475\n",
      "\n",
      "question: Where was the Auction held?\n",
      "washed_answer: The auction was held at the Hard Rock Cafe in New York's Times Square\n",
      "\n",
      "question: What did Venters call Lassiter?\n",
      "washed_answer: Venters called Lassiter a \"gun-man.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f240d6eb21a74f8b8ab64786c382bb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: triviaqa_short\n",
      "Length: 2000\n",
      "Empty Answer: 0\n",
      "Samples:\n",
      "question: Do You Know Where You're Going To? was the theme from which film?\n",
      "washed_answer: mahogany\n",
      "\n",
      "question: Which actor had a Doberman Pinscher called Kirk?\n",
      "washed_answer: william shatner\n",
      "\n",
      "question: Which musical featured the song Thank Heaven for Little Girls?\n",
      "washed_answer: gigi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d46aa11add584a3fa29e0a67b9a7a065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: triviaqa_long\n",
      "Length: 2000\n",
      "Empty Answer: 2\n",
      "Samples:\n",
      "question: Do You Know Where You're Going To? was the theme from which film?\n",
      "washed_answer: Mahogany\n",
      "\n",
      "question: Which actor had a Doberman Pinscher called Kirk?\n",
      "washed_answer: William Shatner\n",
      "\n",
      "question: Which musical featured the song Thank Heaven for Little Girls?\n",
      "washed_answer: Gigi\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3625464900194bf0a06c9f9a7e4e65b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: medmcqa_short\n",
      "Length: 2000\n",
      "Empty Answer: 2\n",
      "Samples:\n",
      "question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma\n",
      "washed_answer: Hyperplasia\n",
      "\n",
      "question: Which vitamin is supplied from only animal source:\n",
      "washed_answer: Vitamin B12 is supplied from only animal source\n",
      "\n",
      "question: All of the following are surgical options for morbid obesity except -\n",
      "washed_answer: Adjustable gastric banding, Biliopancreatic diversion, Duodenal Switch, Roux en Y Duodenal By pass are all surgical options for morbid obesity\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb889b01ae243e98797c3bb167a3f7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: medmcqa_long\n",
      "Length: 2000\n",
      "Empty Answer: 1\n",
      "Samples:\n",
      "question: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma\n",
      "washed_answer: Chronic urethral obstruction due to benign prismatic hyperplasia can lead to the following change in kidney parenchyma:\n",
      "\n",
      "question: Which vitamin is supplied from only animal source:\n",
      "washed_answer: Vitamin B12 is supplied only from animal source\n",
      "\n",
      "question: All of the following are surgical options for morbid obesity except -\n",
      "washed_answer: The correct answer is: Biliopancreatic diversion\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a31025e995546828231b4ec4aa62477",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MedQA-USMLE-4-options_short\n",
      "Length: 2000\n",
      "Empty Answer: 1\n",
      "Samples:\n",
      "question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      "washed_answer: Nitrofurantoin\n",
      "\n",
      "question: A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\n",
      "washed_answer: Placing the infant in a supine position on a firm mattress while sleeping\n",
      "\n",
      "question: A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\n",
      "washed_answer: Abnormal migration of ventral pancreatic bud\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5a735d51967488192e06038580e6120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: MedQA-USMLE-4-options_long\n",
      "Length: 2000\n",
      "Empty Answer: 0\n",
      "Samples:\n",
      "question: A 23-year-old pregnant woman at 22 weeks gestation presents with burning upon urination. She states it started 1 day ago and has been worsening despite drinking more water and taking cranberry extract. She otherwise feels well and is followed by a doctor for her pregnancy. Her temperature is 97.7°F (36.5°C), blood pressure is 122/77 mmHg, pulse is 80/min, respirations are 19/min, and oxygen saturation is 98% on room air. Physical exam is notable for an absence of costovertebral angle tenderness and a gravid uterus. Which of the following is the best treatment for this patient?\n",
      "washed_answer: The best treatment for this patient with symptoms of a urinary tract infection (UTI) would be a course of antibiotics\n",
      "\n",
      "question: A 3-month-old baby died suddenly at night while asleep. His mother noticed that he had died only after she awoke in the morning. No cause of death was determined based on the autopsy. Which of the following precautions could have prevented the death of the baby?\n",
      "washed_answer: Placing the infant in a supine position on a firm mattress while sleeping could have prevented the death of the baby\n",
      "\n",
      "question: A mother brings her 3-week-old infant to the pediatrician's office because she is concerned about his feeding habits. He was born without complications and has not had any medical problems up until this time. However, for the past 4 days, he has been fussy, is regurgitating all of his feeds, and his vomit is yellow in color. On physical exam, the child's abdomen is minimally distended but no other abnormalities are appreciated. Which of the following embryologic errors could account for this presentation?\n",
      "washed_answer: The most likely embryologic error that could account for this presentation is:\n"
     ]
    }
   ],
   "source": [
    "# Show Data Sample\n",
    "model_name = 'vicuna-7b-v1.1'\n",
    "hooked_transformer_name = get_hooked_transformer_name(model_name)\n",
    "hf_model_path = os.path.join(os.environ[\"my_models_dir\"], model_name)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n",
    "hf_tokenizer.pad_token_id = hf_tokenizer.eos_token_id\n",
    "for dst_name in dst_names:\n",
    "    for dst_type in dst_types:\n",
    "        dst = Dataset.load_from_disk(get_cached_result_path('vicuna-7b-v1.1', dst_name, dst_type, 'train'))\n",
    "        dst = dst.map(wash_answer,fn_kwargs=dict(tokenizer=hf_tokenizer))\n",
    "        print(f\"Dataset: {dst_name}_{dst_type}\")\n",
    "        print(f\"Length: {len(dst)}\")\n",
    "        print(f\"Empty Answer: {sum([1 if len(x) == 0 else 0 for x in dst['washed_answer']])}\")\n",
    "        print(f\"Samples:\")\n",
    "        for i in range(3):\n",
    "            for k in ['question','washed_answer']:\n",
    "                print(f\"{k}: {dst[k][i]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbcb7d017f0b9170",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T15:30:49.082399Z",
     "start_time": "2024-04-08T15:30:47.770176Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e949d7d5e34420ca9d6697d87f956a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/20000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Merged Dataset to cached_results/vicuna-7b-v1.1/long/all_train\n",
      "1000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aba1d412ae4494093a5c743821e39e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save Merged Dataset to cached_results/vicuna-7b-v1.1/long/all_validation\n"
     ]
    }
   ],
   "source": [
    "# Merge Train Dataset\n",
    "model_name = 'vicuna-7b-v1.1'\n",
    "train_size_per_dataset = 2000\n",
    "val_size_per_dataset = 100\n",
    "\n",
    "\n",
    "def merge_dst(model_name, dst_names, dst_types, dst_split, data_size):\n",
    "    all_dst = [Dataset.load_from_disk(get_cached_result_path(model_name, dst_name, dst_type, dst_split)) for dst_name in dst_names for dst_type in dst_types]\n",
    "\n",
    "    all_columns = ['dst_template', 'question', 'input', 'input_ids', 'gt', 'options', 'answer_ids']\n",
    "\n",
    "    def fill_missing_columns(dst: Dataset):\n",
    "        for column in dst.column_names:\n",
    "            if column not in all_columns:\n",
    "                dst = dst.remove_columns(column)\n",
    "        if 'options' not in dst.column_names:\n",
    "            dst = dst.add_column('options', [['#####'] for i in range(len(dst))])\n",
    "        return dst\n",
    "\n",
    "    all_dst = [dst if len(dst) <= data_size else dst.select(range(data_size)) for dst in all_dst]\n",
    "    all_dst = [fill_missing_columns(dst) for dst in all_dst]\n",
    "    merged_dst = datasets.concatenate_datasets(all_dst)\n",
    "    print(len(merged_dst))\n",
    "    save_path = get_cached_result_path(model_name, 'all', 'long', dst_split)\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    merged_dst.save_to_disk(save_path)\n",
    "    print(f\"Save Merged Dataset to {save_path}\")\n",
    "\n",
    "merge_dst(model_name, dst_names, dst_types, 'train', train_size_per_dataset)\n",
    "merge_dst(model_name, dst_names, dst_types, 'validation', val_size_per_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e02693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Results: Get Main Results\n",
    "\n",
    "c_metric = 'include'\n",
    "c_th = 0.3\n",
    "\n",
    "model_names_index = [model_names_alias[name] for name in model_names for i in range(len(dst_names))]\n",
    "dst_names_index = [dst_names_alias[name] for name in dst_names] * len(model_names)\n",
    "multi_index = pd.MultiIndex.from_tuples(zip(model_names_index, dst_names_index), names=['Model', 'Dataset'])\n",
    "columns = ['ACC'] + [alias for u_metric, alias in u_metric_alias.items() if (c_metric in u_metric) or 'ours' not in u_metric]\n",
    "main_results_short = pd.DataFrame(columns=columns, index=multi_index).astype(float)\n",
    "main_results_long = deepcopy(main_results_short)\n",
    "\n",
    "for model_name in model_names:\n",
    "    for dst_name in dst_names:\n",
    "        for dst_type in dst_types:\n",
    "            new_row = {k: 0. for k in main_results_short.columns}\n",
    "            result_path = get_eval_main_result_path(model_name, dst_name, dst_type)\n",
    "            if os.path.exists(result_path):\n",
    "                test_dst = Dataset.load_from_disk(result_path)\n",
    "                new_row['ACC'] = get_acc_by_c_th(test_dst, c_metric, c_th) * 100\n",
    "                for u_metric, u_metric_name in u_metric_alias.items():\n",
    "                    if u_metric_name in columns and u_metric in test_dst.column_names:\n",
    "                        new_row[u_metric_name] = get_auroc(test_dst, u_metric, c_metric, c_th) * 100\n",
    "            result = main_results_short if dst_type == 'short' else main_results_long\n",
    "            result.loc[(model_names_alias[model_name], dst_names_alias[dst_name])] = new_row\n",
    "\n",
    "print(f\"Correctness Metric: {c_metric} Threshold: {c_th} AUROC Results\")\n",
    "print(\"Short Prompt Main Result:\")\n",
    "display(main_results_short)\n",
    "\n",
    "print(\"Long Prompt Main Result:\")\n",
    "display(main_results_long)\n",
    "\n",
    "# print(main_results_short.to_latex(index=True, float_format=\"%.2f\"))\n",
    "# print(main_results_long.to_latex(index=True, float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635415cfed8278ee",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generalization1: Cross Dataset and Cross Prompt Evaluation\n",
    "model_name = 'vicuna-7b-v1.1'\n",
    "\n",
    "\n",
    "def plot_cross_dst_matrix(model_name, u_metric, c_metric, c_th):\n",
    "    fig_index = [(name, type) for type in dst_types for name in dst_names]\n",
    "    fig_axis = list(map(lambda idx: f\"{dst_names_alias[idx[0]]}-{idx[1]}\", fig_index))\n",
    "    cross_eval_matrix = torch.zeros(len(dst_types) * len(dst_names), len(dst_types) * len(dst_names))\n",
    "    fig = go.Figure()\n",
    "    annotations = []\n",
    "\n",
    "    for i, (train_dst_name, train_dst_type) in enumerate(fig_index):\n",
    "        for j, (test_dst_name, test_dst_type) in enumerate(fig_index):\n",
    "            result_path = get_eval_cross_result_path(model_name, train_dst_name, train_dst_type, test_dst_name, test_dst_type, u_metric.split(\"_\")[-1])\n",
    "            if os.path.exists(f\"{result_path}/dataset_info.json\"):\n",
    "                cross_eval_result = Dataset.load_from_disk(result_path)\n",
    "                cross_eval_matrix[j][i] = get_auroc(cross_eval_result, u_metric, c_metric, c_th) * 100\n",
    "                annotations.append(dict(\n",
    "                    x=i,\n",
    "                    y=j,\n",
    "                    text=f\"{cross_eval_matrix[j][i].item():.2f}\",\n",
    "                    showarrow=False,\n",
    "                    font=dict(\n",
    "                        color='white'\n",
    "                    )\n",
    "                ))\n",
    "    fig.add_trace(go.Heatmap(z=cross_eval_matrix, x=fig_axis, y=fig_axis, colorscale='Inferno'))\n",
    "    fig.update_layout(\n",
    "        title_text=f\"Model: {model_names_alias[model_name]} Correctness Metric: {c_metric} Method: {u_metric} Cross Eval Results\",\n",
    "        xaxis_title=\"Train Dataset\",\n",
    "        yaxis_title=\"Test Dataset\",\n",
    "        width=1000,\n",
    "        height=1000,\n",
    "        annotations=annotations\n",
    "    )\n",
    "    fig.show()\n",
    "    overall_average_drop = cross_eval_matrix.mean().item() - cross_eval_matrix.diag().mean().item()\n",
    "    short_average_drop = cross_eval_matrix[:5, :5].mean().item() - cross_eval_matrix[:5, :5].diag().mean().item()\n",
    "    long_average_drop = cross_eval_matrix[5:, 5:].mean().item() - cross_eval_matrix[5:, 5:].diag().mean().item()\n",
    "    cross_dst_average_drop = (short_average_drop + long_average_drop) / 2\n",
    "    cross_prompt_average_drop = 0\n",
    "    for i in range(len(cross_eval_matrix)):\n",
    "        cross_prompt_average_drop += cross_eval_matrix[i][(i + len(cross_eval_matrix) // 2) % len(cross_eval_matrix)].item()\n",
    "    cross_prompt_average_drop /= len(cross_eval_matrix)\n",
    "    print(f\"Overall Average Drop: {overall_average_drop:.2f}\")\n",
    "    print(f\"Short Average Drop: {short_average_drop:.2f}\")\n",
    "    print(f\"Long Average Drop: {long_average_drop:.2f}\")\n",
    "    print(f\"Cross Dst Average Drop: {cross_dst_average_drop:.2f}\")\n",
    "    print(f\"Cross Prompt Average Drop: {cross_prompt_average_drop:.2f}\")\n",
    "    for i in range(len(cross_eval_matrix)):\n",
    "        dst_drop = cross_eval_matrix[:, i].mean().item() - cross_eval_matrix[i, i].item()\n",
    "        print(f\"Train Dst:{fig_axis[i]} Average Drop: {dst_drop:.2f}\")\n",
    "\n",
    "\n",
    "plot_cross_dst_matrix(model_name, u_metric='u_score_ours_mean_soft_rougel', c_metric='include', c_th=0.3)\n",
    "plot_cross_dst_matrix(model_name, u_metric='u_score_ours_mean_soft_include', c_metric='include', c_th=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae1f8fbb39774e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generalization2: Cross Correctness Metric Evaluation\n",
    "model_name = 'vicuna-7b-v1.1'\n",
    "c_th = 0.3\n",
    "\n",
    "c_metric_index = [name for name in c_metrics for i in range(len(dst_names))]\n",
    "dst_names_index = [dst_names_alias[name] for name in dst_names] * len(c_metrics)\n",
    "multi_index = pd.MultiIndex.from_tuples(zip(c_metric_index, dst_names_index), names=['Correctness Metric', 'Dataset'])\n",
    "columns = ['ACC'] + [u for u in u_metric_alias.values() if 'Ours' in u]\n",
    "cross_c_metric_results_short = pd.DataFrame(columns=columns, index=multi_index).astype(float)\n",
    "cross_c_metric_results_long = deepcopy(cross_c_metric_results_short)\n",
    "\n",
    "for dst_type in dst_types:\n",
    "    result = cross_c_metric_results_short if dst_type == 'short' else cross_c_metric_results_long\n",
    "    for c_metric in c_metrics:\n",
    "        for dst_name in dst_names:\n",
    "            new_row = {k: 0. for k in columns}\n",
    "            result_path = get_eval_main_result_path(model_name, dst_name, dst_type)\n",
    "            if os.path.exists(result_path):\n",
    "                test_dst = Dataset.load_from_disk(result_path)\n",
    "                new_row['ACC'] = get_acc_by_c_th(test_dst, c_metric, c_th) * 100\n",
    "                for u_metric, u_metric_name in u_metric_alias.items():\n",
    "                    if u_metric_name in columns and u_metric in test_dst.column_names:\n",
    "                        new_row[u_metric_name] = get_auroc(test_dst, u_metric, c_metric, c_th) * 100\n",
    "            result.loc[(c_metric, dst_names_alias[dst_name])] = new_row\n",
    "\n",
    "print(\"Short Prompt Cross Correctness Metric Result:\")\n",
    "display(cross_c_metric_results_short)\n",
    "\n",
    "print(\"Long Prompt Cross Correctness Metric Result:\")\n",
    "display(cross_c_metric_results_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd8d7aba27e1c5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Efficiency: Get Efficiency Results\n",
    "model_name = 'vicuna-7b-v1.1'\n",
    "dst_name = 'sciq'\n",
    "dst_type = 'long'\n",
    "\n",
    "\n",
    "def plot_efficiency_results(model_name, dst_name, dst_type):\n",
    "    fig = go.Figure()\n",
    "    result_path = get_eval_main_result_path(model_name, dst_name, dst_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897bab3f385d51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Correctness Metric Similarity\n",
    "base_c_metric = \"rougel\"\n",
    "model_name = 'vicuna-7b-v1.1'\n",
    "\n",
    "for dst_name in dst_names:\n",
    "    for dst_type in dst_types:\n",
    "        result_path = get_eval_main_result_path(model_name, dst_name, dst_type)\n",
    "        if os.path.exists(result_path):\n",
    "            test_dst = Dataset.load_from_disk(result_path).select(range(500))\n",
    "            # test_dst = test_dst.add_column(\"idx\", list(range(len(test_dst))))\n",
    "            test_dst = test_dst.sort(base_c_metric)\n",
    "            test_dst = test_dst.map(lambda x: dict(include=x['include'] - 0.04) if x['include'] == 0 else dict(include=x['include'] + 0.04))\n",
    "            test_dst = test_dst.map(lambda x: dict(sentsim=x['sentsim'] + 0.02) if x['sentsim'] > 0.99 else dict(sentsim=x['sentsim']))\n",
    "            # test_dst = test_dst.map(lambda x: dict(rougel=x['rougel']+0.02) if x['rougel'] > 0.99 else dict(rougel=x['rougel']))\n",
    "            fig = go.Figure()\n",
    "            for c_metric in c_metrics:\n",
    "                fig.add_trace(go.Scatter(x=list(range(len(test_dst))), y=test_dst[c_metric], mode='markers', name=c_metric))\n",
    "            fig.update_layout(title_text=f\"Model: {model_names_alias[model_name]} Dataset: {dst_names_alias[dst_name]}-{dst_type} Correctness Metric Similarity\",\n",
    "                              xaxis_title=base_c_metric,\n",
    "                              yaxis_title=\"Other Correctness Metric\",\n",
    "                              width=2000,\n",
    "                              height=1000)\n",
    "            fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b05237749c9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged Training: Get Merged Training Eval Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b5388f4cdde32",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ablation Study: Get Ablation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f82d29861e8a8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sensitivity Analysis : Get Sensitivity Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db69225b79de49",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Case Study: show token level u_score\n",
    "example = test_dst.filter(lambda x: x['rougel'] < 0.1)[1]\n",
    "example = test_dst[2]\n",
    "print(f\"gt:{example['gt']}\")\n",
    "print(f\"options:{example['options']}\")\n",
    "\n",
    "str_tokens = model.to_str_tokens(f\":{example['washed_answer']}\", prepend_bos=False)[1:]\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Token Level\", \"Sentence Level\"), row_heights=[0.5, 0.5])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(str_tokens))), y=example['u_score_pe_all'], mode='lines+markers'), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Token', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=1, col=1)\n",
    "\n",
    "sentence_u_score_pe_all = []\n",
    "indices = [0] + [i for i, x in enumerate(str_tokens) if x == '.'] + [-1]\n",
    "spans = [(indices[i], indices[i + 1]) for i in range(len(indices) - 1)]\n",
    "print(len(indices))\n",
    "for span in spans:\n",
    "    sentence_score = sum(example['u_score_pe_all'][span[0]:span[1]]) / (span[1] - span[0])\n",
    "    sentence_u_score_pe_all.extend([sentence_score] * (span[1] - span[0]))\n",
    "sentence_u_score_pe_all.append(sentence_u_score_pe_all[-1])\n",
    "# print(str_tokens)\n",
    "for i, sentence in enumerate(example['washed_answer'].split(\".\")):\n",
    "    print(i + 1, sentence.replace(\"\\n\", ' ').strip())\n",
    "# print(len(example['u_score_pe_all']))\n",
    "# print(sentence_u_score_pe_all)\n",
    "# print(len(sentence_u_score_pe_all))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(str_tokens))), y=sentence_u_score_pe_all, mode='lines+markers'), row=2, col=1)\n",
    "fig.update_xaxes(title_text='Sentence', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=1000, width=2500, margin=dict(l=0, r=0, b=50, t=50), title_text=example['washed_answer'])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
