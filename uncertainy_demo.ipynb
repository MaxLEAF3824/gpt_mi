{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3ad78fe71283d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inplementation of Internal State-based Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278c24516568ba0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 数据集预处理：将不同格式的数据集处理成input+gt的格式，方便判断模型的correctness，这一部分的采用固定不可调整的prompt，即Context: Question: Options: Answer:格式\n",
    "2. 生成回复，为每个模型确定一个prompt，一个max_new_tokens数，然后生成回复\n",
    "3. 计算回复部分的correctness指标，判断模型的回复是否正确\n",
    "4. 计算uncertainty指标，包括PE, LN-PE, SAR, Ours\n",
    "5. 计算AUROC，绘制AUROC/Correctness-Threshold曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T09:11:30.749390Z",
     "start_time": "2024-04-02T09:11:30.745621Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb36a0ca3d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import *\n",
    "\n",
    "datasets.disable_caching()\n",
    "torch.set_grad_enabled(False)\n",
    "# print_sys_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea1457e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T09:00:29.056983Z",
     "start_time": "2024-04-02T09:00:29.049701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Eval Result Config\n",
    "model_names = [\n",
    "    \"vicuna-7b-v1.1\",\n",
    "    \"vicuna-13b-v1.1\",\n",
    "    \"vicuna-33b-v1.3\",\n",
    "]\n",
    "dst_names = [\n",
    "    \"sciq\",\n",
    "    \"coqa\",\n",
    "    \"triviaqa\",\n",
    "    \"medmcqa\",\n",
    "    \"MedQA-USMLE-4-options\",\n",
    "]\n",
    "\n",
    "c_metrics = [\n",
    "    'rougel'\n",
    "    'sentsim'\n",
    "    'include'\n",
    "]\n",
    "\n",
    "dst_types = [\n",
    "    \"long\",\n",
    "    \"short\"\n",
    "]\n",
    "\n",
    "acc_map={\n",
    "    \"vicuna-7b-v1.1\": {\n",
    "        \"sciq\" : 0.65,\n",
    "        \"coqa\" : 0.0,\n",
    "        \"triviaqa\" : 0.55,\n",
    "        \"medmcqa\" : 0.0,\n",
    "        \"MedQA-USMLE-4-options\" : 0.0\n",
    "    },\n",
    "    \"vicuna-13b-v1.1\": {\n",
    "        \"sciq\" : 0.0,\n",
    "        \"coqa\" : 0.0,\n",
    "        \"triviaqa\" : 0.0,\n",
    "        \"medmcqa\" : 0.0,\n",
    "        \"MedQA-USMLE-4-options\" : 0.0\n",
    "    },\n",
    "    \"vicuna-33b-v1.3\": {\n",
    "        \"sciq\" : 0.0,\n",
    "        \"coqa\" : 0.0,\n",
    "        \"triviaqa\" : 0.0,\n",
    "        \"medmcqa\" : 0.0,\n",
    "        \"MedQA-USMLE-4-options\" : 0.0\n",
    "    }\n",
    "}\n",
    "\n",
    "cth_map = dict(\n",
    "    rougel=dict(\n",
    "        coqa_long=1\n",
    "    )\n",
    ")\n",
    "\n",
    "def get_eval_main_result_path(model_name, dst_name, dst_type):\n",
    "    return f\"eval_results/{model_name}/{dst_name}_{dst_type}\"\n",
    "\n",
    "def get_eval_cross_result_path(model_name, train_dst_name, train_dst_type, test_dst_name, test_dst_type):\n",
    "    return f\"cross_eval_results/{model_name}/rougel/v_c_{train_dst_name}_{train_dst_type}_mean_soft_best.pth/{test_dst_name}_{test_dst_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17ba7314",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-02T09:00:09.807580Z",
     "start_time": "2024-04-02T09:00:09.756677Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gt: Tell the attending that he cannot fail to disclose this mistake\n",
      "gt: Cross-linking of DNA\n",
      "gt: Cholesterol embolization\n",
      "gt: Lactose-fermenting, gram-negative rods forming pink colonies on MacConkey agar\n",
      "gt: Ketotifen eye drops\n",
      "gt: Reassurance and continuous monitoring\n",
      "gt: Common iliac artery aneurysm\n",
      "gt: Clopidogrel\n",
      "gt: Active or recurrent pelvic inflammatory disease (PID)\n",
      "gt: Silvery plaques on extensor surfaces\n",
      "gt: It is an HIV-1/HIV2 antibody differentiation immunoassay\n",
      "gt: Ruxolitinib\n",
      "gt: Meningioma\n",
      "gt: A reduction in diastolic filling time\n",
      "gt: Rotavirus\n",
      "gt: Gallbladder cancer\n",
      "gt: IL-4\n",
      "gt: Matching\n",
      "gt: Ibuprofen + colchicine +/- omeprazole\n",
      "gt: Benzodiazepine intoxication\n",
      "\"\n",
      "gt: Previous radiation therapy\n",
      "gt: 22q11 deletion\n",
      "gt: Histoplasma capsulatum infection\n",
      "gt: Staphylococcus aureus\n",
      "gt: Intubate with mechanical ventilation\n",
      "gt: Respiratory burst\n",
      "gt: Steeple sign\n",
      "gt: Induction of CYP3A4 by rifampin leading to decreased serum levels of ethinylestradiol and progesterone\n",
      "gt: Increased cerebrospinal fluid protein with normal cell count\n",
      "gt: Reassurance\n"
     ]
    }
   ],
   "source": [
    "test_dst = Dataset.load_from_disk(get_eval_main_result_path('vicuna-7b-v1.1', 'MedQA-USMLE-4-options', 'long'))\n",
    "for i in range(0,30):\n",
    "    for k in ['gt']:\n",
    "        print(f\"{k}: {test_dst[i][k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702b3ebfcfa6944",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Model Config\n",
    "# model_name = \"vicuna-7b-v1.1\"\n",
    "# hooked_transformer_name = \"llama-7b-hf\"\n",
    "# hf_model_path = os.path.join(os.environ[\"my_models_dir\"], model_name)\n",
    "# hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n",
    "# hf_model = AutoModelForCausalLM.from_pretrained(hf_model_path)\n",
    "# \n",
    "# model = HookedTransformer.from_pretrained_no_processing(hooked_transformer_name, dtype='bfloat16', hf_model=hf_model, tokenizer=hf_tokenizer, default_padding_side='left')\n",
    "# \n",
    "# # Aux Models\n",
    "# se_bert_name = \"microsoft/deberta-large-mnli\"\n",
    "# nli_pipe = pipeline(\"text-classification\", model=se_bert_name, device=0)\n",
    "# \n",
    "# # sar_bert_name = 'cross-encoder/stsb-roberta-large'\n",
    "# # # sar_bert_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "# # sar_bert = SentenceTransformer(sar_bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88edd9cca2e9bbfb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our Method OLD\n",
    "def compute_certainty_vector_mean(train_dst, layers, act_name, batch_size=8, model):\n",
    "    def get_paired_dst_sciq(train_dst):\n",
    "        tmp_pos = \"Question:{q} Options:{o} The correct answer is:\"\n",
    "        tmp_neg = \"Question:{q} Options:{o} The incorrect answer is:\"\n",
    "    \n",
    "        # sciq_train_dst = sciq_train_dst.filter(lambda x: x['rougel'] > 0.5)\n",
    "    \n",
    "        def get_pos_example(example):\n",
    "            example['input'] = tmp_pos.format(q=example['question'], o=\", \".join(example['options']))\n",
    "            example['washed_output'] = f\"{example['input']}{example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            example['input'] = tmp_neg.format(q=example['question'], o=\", \".join(example['options']))\n",
    "            wrong_options = [opt for opt in example['options'] if opt != example['gt']]\n",
    "            if wrong_options:\n",
    "                random.seed(42 + idx)\n",
    "                wrong_answer = random.choice(wrong_options)\n",
    "            else:\n",
    "                wrong_answer = \"wrong answer\"\n",
    "            example['washed_output'] = f\"{example['input']}{wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    \n",
    "    def get_paired_dst_coqa(train_dst):\n",
    "        def get_pos_example(example):\n",
    "            example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            wrong_options = [opt for opt in example['answers']['input_text'] if opt != example['gt']]\n",
    "            if wrong_options:\n",
    "                random.seed(42 + idx)\n",
    "                wrong_answer = random.choice(wrong_options)\n",
    "            else:\n",
    "                wrong_answer = \"wrong answer\"\n",
    "            example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    \n",
    "    def get_paired_dst_triviaqa(train_dst):\n",
    "        def get_pos_example(example):\n",
    "            example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            next_idx = idx + 1 if idx + 1 < len(train_dst) else 0\n",
    "            wrong_answer = train_dst[next_idx]['gt']\n",
    "            example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    \n",
    "    def get_paired_dst_medmcqa(train_dst):\n",
    "        def get_pos_example(example):\n",
    "            example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "            return example\n",
    "    \n",
    "        def get_neg_example(example, idx):\n",
    "            wrong_options = [opt for opt in example['options'] if opt != example['gt']]\n",
    "            if wrong_options:\n",
    "                random.seed(42 + idx)\n",
    "                wrong_answer = random.choice(wrong_options)\n",
    "            else:\n",
    "                wrong_answer = \"wrong answer\"\n",
    "            example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "            return example\n",
    "    \n",
    "        dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "        dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "        return dst_pos, dst_neg\n",
    "    \n",
    "    func_map = {\n",
    "        'allenai/sciq': get_paired_dst_sciq,\n",
    "        'stanfordnlp/coqa': get_paired_dst_coqa,\n",
    "        'lucadiliello/triviaqa': get_paired_dst_triviaqa,\n",
    "        'openlifescienceai/medmcqa': get_paired_dst_medmcqa,\n",
    "        'GBaker/MedQA-USMLE-4-options': get_paired_dst_medmcqa\n",
    "    }\n",
    "    func = func_map[dst_name]\n",
    "    dst_pos, dst_neg = func(train_dst)\n",
    "    \n",
    "    data_pos = dst_pos['washed_output']\n",
    "    data_neg = dst_neg['washed_output']\n",
    "    data_size = len(data_pos)\n",
    "    full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "    v_c = torch.zeros((len(layers), 1, model.cfg.d_model)).cuda()\n",
    "\n",
    "    for i in tqdm(range(0, data_size, batch_size)):\n",
    "        batch_pos = data_pos[i:i + batch_size]\n",
    "        batch_neg = data_neg[i:i + batch_size]\n",
    "\n",
    "        _, cache_pos = model.run_with_cache(batch_pos, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "        _, cache_neg = model.run_with_cache(batch_neg, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "\n",
    "        cache_pos = einops.rearrange([cache_pos[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "        cache_neg = einops.rearrange([cache_neg[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "\n",
    "        cache_pos = cache_pos[:, :, [-1], :]\n",
    "        cache_neg = cache_neg[:, :, [-1], :]\n",
    "\n",
    "        v_c += (cache_pos.sum(dim=0) - cache_neg.sum(dim=0))\n",
    "\n",
    "    v_c /= data_size\n",
    "\n",
    "    v_c = v_c.cpu().float()\n",
    "    v_c = F.normalize(v_c, p=2, dim=-1)\n",
    "    return v_c\n",
    "\n",
    "# clean_exp exp\n",
    "def clean_exp(dst, v_c, layers, act_name):\n",
    "    fig = go.Figure()\n",
    "    c_scores = []\n",
    "    w_scores = []\n",
    "    labels = []\n",
    "    u_scores = []\n",
    "    u_scores_z = []\n",
    "    all_pe_u_scores = []\n",
    "    all_ln_pe_u_scores = []\n",
    "\n",
    "    def batch_get_result(examples):\n",
    "        all_outputs = []\n",
    "        all_num_answer_tokens = []\n",
    "        all_num_input_tokens = list(map(len, model.to_str_tokens(examples['input'])))\n",
    "        bsz = len(examples['input'])\n",
    "\n",
    "        for i in range(bsz):\n",
    "            example = {k: examples[k][i] for k in examples.keys()}\n",
    "            if example.get(\"options\"):\n",
    "                wrong_options = [opt for opt in example['options']]\n",
    "                for opt in wrong_options:\n",
    "                    if opt == example['gt']:\n",
    "                        wrong_options.remove(opt)\n",
    "                        break\n",
    "            elif example.get(\"answers\"):\n",
    "                wrong_options = [opt for opt in example['answers']['input_text']]\n",
    "                for opt in wrong_options:\n",
    "                    if opt == example['gt']:\n",
    "                        wrong_options.remove(opt)\n",
    "                        break\n",
    "                wrong_options = wrong_options[:3]\n",
    "            else:\n",
    "                wrong_options = ['wrong answer', 'bad answer', 'incorrect answer']\n",
    "            correct_output = example['input'] + example['gt']\n",
    "            wrong_outputs = [example['input'] + opt for opt in wrong_options]\n",
    "            all_outputs.extend([correct_output] + wrong_outputs)\n",
    "            num_answer_tokens = list(map(len, model.to_str_tokens([example['gt']] + wrong_options)))\n",
    "            all_num_answer_tokens.append(num_answer_tokens)\n",
    "\n",
    "        full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "\n",
    "        batch_logits, batch_cache = model.run_with_cache(all_outputs, names_filter=lambda x: x in full_act_names,\n",
    "                                                         device='cpu',\n",
    "                                                         padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "        batch_cache = einops.rearrange([batch_cache[name] for name in full_act_names],\n",
    "                                       'l b p d -> b l p d').float().cpu()\n",
    "        batch_cache = einops.rearrange(batch_cache, '(b o) l p d -> b o l p d', o=4)\n",
    "        batch_cache = batch_cache[:, :, :, [-1], :]\n",
    "\n",
    "        batch_logits = batch_logits.cpu().float()\n",
    "        batch_logits = einops.rearrange(batch_logits, '(b o) p v -> b o p v', o=4)\n",
    "\n",
    "        for i, lg_4 in enumerate(batch_logits):\n",
    "            num_answer_tokens = all_num_answer_tokens[i]\n",
    "            num_input_tokens = all_num_input_tokens[i]\n",
    "            for j, lg in enumerate(lg_4):\n",
    "                output = all_outputs[i * 4 + j]\n",
    "                answer_lg = lg[-num_answer_tokens[j] - 1:-1]\n",
    "                answer_prob = F.softmax(answer_lg, dim=-1)\n",
    "                answer_target_prob = answer_prob.max(dim=-1).values\n",
    "                pe = -torch.log(answer_target_prob).sum().item()\n",
    "                # print(f\"pe:{pe}\")\n",
    "                ln_pe = -torch.log(answer_target_prob).mean().item()\n",
    "                # print(f\"ln_pe:{ln_pe}\")\n",
    "                all_pe_u_scores.append(pe)\n",
    "                all_ln_pe_u_scores.append(ln_pe)\n",
    "\n",
    "        batch_in_vivo_auroc = []\n",
    "        for i in range(bsz):\n",
    "            cache = batch_cache[i]\n",
    "            u_score = einsum('b l p d, l p d -> b', cache, v_c)\n",
    "            u_score_z = (u_score - u_score.mean()) / u_score.std()\n",
    "\n",
    "            u_score = u_score.tolist()\n",
    "            u_score_z = u_score_z.tolist()\n",
    "\n",
    "            in_vivo_auroc = roc_auc_score([1, 0, 0, 0], u_score)\n",
    "            batch_in_vivo_auroc.append(in_vivo_auroc)\n",
    "            # if u_score[0] > max(u_score[1:]):\n",
    "            #     batch_in_vivo_auroc.append(1)\n",
    "            # else:\n",
    "            #     batch_in_vivo_auroc.append(0)\n",
    "\n",
    "            c_scores.append(u_score_z[0])\n",
    "            w_scores.extend(u_score_z[1:])\n",
    "            labels.extend([1, 0, 0, 0])\n",
    "\n",
    "            # assert len(u_score) == 4, f\"{len(u_score)} {example['options']}\"\n",
    "            u_scores.extend(u_score)\n",
    "            u_scores_z.extend(u_score_z)\n",
    "\n",
    "        examples['in_vivo_auroc'] = batch_in_vivo_auroc\n",
    "        return examples\n",
    "\n",
    "    new_dst = dst.map(batch_get_result, new_fingerprint=str(time()), batched=True, batch_size=4)\n",
    "\n",
    "    in_vivo_auroc = sum(new_dst['in_vivo_auroc']) / len(new_dst['in_vivo_auroc'])\n",
    "    flag = in_vivo_auroc > 0.5\n",
    "    in_vivo_auroc = in_vivo_auroc if flag else 1 - in_vivo_auroc\n",
    "    print(f\"in-vivo u_score auroc: {in_vivo_auroc}\")\n",
    "\n",
    "    in_vitro_auroc = roc_auc_score(labels, u_scores)\n",
    "    in_vitro_auroc = in_vitro_auroc if flag else 1 - in_vitro_auroc\n",
    "    print(f\"in-vitro u_score auroc: {in_vitro_auroc}\")\n",
    "\n",
    "    in_vitro_auroc_z = roc_auc_score(labels, u_scores_z)\n",
    "    in_vitro_auroc_z = in_vitro_auroc_z if flag else 1 - in_vitro_auroc_z\n",
    "    print(f\"in-vitro u_score_z auroc: {in_vitro_auroc_z}\")\n",
    "\n",
    "    in_vitro_pe_auroc = roc_auc_score(labels, all_pe_u_scores)\n",
    "    print(f\"in-vitro pe auroc: {in_vitro_pe_auroc}\")\n",
    "\n",
    "    in_vitro_ln_pe_auroc = roc_auc_score(labels, all_ln_pe_u_scores)\n",
    "    print(f\"in-vitro ln_pe auroc: {in_vitro_ln_pe_auroc}\")\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=c_scores, name='Correct', opacity=0.5, nbinsx=100))\n",
    "    fig.add_trace(go.Histogram(x=w_scores, name='Wrong', opacity=0.5, nbinsx=100))\n",
    "    fig.update_layout(barmode='overlay')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db69225b79de49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-21T15:20:35.189620Z",
     "start_time": "2024-03-21T15:20:35.065244Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "example = test_dst.filter(lambda x: x['rougel'] < 0.1)[1]\n",
    "example = test_dst[2]\n",
    "print(f\"gt:{example['gt']}\")\n",
    "print(f\"options:{example['options']}\")\n",
    "\n",
    "str_tokens = model.to_str_tokens(f\":{example['washed_answer']}\", prepend_bos=False)[1:]\n",
    "fig = make_subplots(rows=2, cols=1, subplot_titles=(\"Token Level\", \"Sentence Level\"), row_heights=[0.5, 0.5])\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(str_tokens))), y=example['u_score_pe_all'], mode='lines+markers'), row=1, col=1)\n",
    "fig.update_xaxes(title_text='Token', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=1, col=1)\n",
    "\n",
    "sentence_u_score_pe_all = []\n",
    "indices = [0]+[i for i, x in enumerate(str_tokens) if x == '.']+[-1]\n",
    "spans = [(indices[i], indices[i+1]) for i in range(len(indices)-1)]\n",
    "print(len(indices))\n",
    "for span in spans:\n",
    "    sentence_score = sum(example['u_score_pe_all'][span[0]:span[1]]) / (span[1] - span[0])\n",
    "    sentence_u_score_pe_all.extend([sentence_score] * (span[1] - span[0]))\n",
    "sentence_u_score_pe_all.append(sentence_u_score_pe_all[-1])\n",
    "# print(str_tokens)\n",
    "for i, sentence in enumerate(example['washed_answer'].split(\".\")):\n",
    "    print(i+1,sentence.replace(\"\\n\",' ').strip())\n",
    "# print(len(example['u_score_pe_all']))\n",
    "# print(sentence_u_score_pe_all)\n",
    "# print(len(sentence_u_score_pe_all))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=list(range(len(str_tokens))), y=sentence_u_score_pe_all, mode='lines+markers'), row=2, col=1)\n",
    "fig.update_xaxes(title_text='Sentence', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=2, col=1)\n",
    "\n",
    "fig.update_layout(height=1000, width=2500, margin=dict(l=0, r=0, b=50, t=50), title_text=example['washed_answer'])\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
