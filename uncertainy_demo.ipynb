{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:18:46.871038Z",
     "start_time": "2024-03-08T03:18:29.465798Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩余内存: 851.0 G\n",
      "当前主机名是:SH-IDC1-10-140-0-157\n",
      "SH-IDC1-10-140-0-157      Fri Mar  8 11:18:46 2024  525.60.13\n",
      "[0] NVIDIA A100-SXM4-80GB | 25°C,   0 % |     0 / 81920 MB |\n",
      "[1] NVIDIA A100-SXM4-80GB | 45°C,  89 % | 53429 / 81920 MB | huyutao(53426M)\n",
      "[2] NVIDIA A100-SXM4-80GB | 29°C,   0 % |  2423 / 81920 MB | yejin(2420M)\n",
      "[3] NVIDIA A100-SXM4-80GB | 25°C,   0 % |     0 / 81920 MB |\n",
      "[4] NVIDIA A100-SXM4-80GB | 32°C,  36 % | 28955 / 81920 MB | huyutao(28952M)\n",
      "[5] NVIDIA A100-SXM4-80GB | 31°C,   0 % |     0 / 81920 MB |\n",
      "[6] NVIDIA A100-SXM4-80GB | 39°C,   0 % |     0 / 81920 MB |\n",
      "[7] NVIDIA A100-SXM4-80GB | 58°C, 100 % | 66843 / 81920 MB | yejin(66840M)\n",
      "Clash is running, pid: 348131\n",
      "\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"Start initial compatible provider 选择节点\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"Start initial compatible provider 主站加速\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"Start initial compatible provider 广告屏蔽\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"Start initial compatible provider 海外游戏平台\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"Start initial compatible provider NETFLIX\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"Start initial compatible provider Bilibili哔哩哔哩\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"RESTful API listening at: [::]:9090\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"HTTP proxy listening at: [::]:7890\"\n",
      "time=\"2024-03-08T11:18:46+08:00\" level=info msg=\"SOCKS proxy listening at: [::]:7891\"\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple, Union\n",
    "import os\n",
    "import random\n",
    "import rouge_score\n",
    "from rouge_score import rouge_scorer\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from copy import deepcopy\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def print_sys_info():\n",
    "    import psutil\n",
    "    import socket\n",
    "    import gpustat\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(\"剩余内存: {} G\".format(memory.available/1024/1024//1024))\n",
    "    host_name = socket.gethostname()\n",
    "    print(f\"当前主机名是:{host_name}\")\n",
    "    gpustat.print_gpustat()\n",
    "\n",
    "def launch_clash():\n",
    "    import subprocess\n",
    "    import os\n",
    "\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if not result.stdout:\n",
    "        subprocess.Popen(\"~/tools/clash/clash\", shell=True)\n",
    "        result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(f\"Clash is running, pid: {result.stdout}\")\n",
    "    os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "    os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "print_sys_info()\n",
    "launch_clash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a702b3ebfcfa6944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:19:10.210611Z",
     "start_time": "2024-03-08T03:18:46.873290Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f81b7b96825b4251b83ded4707f07bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/guoyiqiu/miniconda3/envs/mi/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b-hf into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "# hf_model_path = \"google/gemma-2b-it\"\n",
    "hf_model_path = os.path.join(os.environ[\"my_models_dir\"], \"vicuna-7b-v1.1\")\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(hf_model_path)\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\"llama-7b-hf\", dtype='bfloat16', hf_model=hf_model, tokenizer=hf_tokenizer, default_padding_side='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76fd5bbe0823b533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:19:15.284449Z",
     "start_time": "2024-03-08T03:19:10.212673Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since allenai/sciq couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since allenai/sciq couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /mnt/petrelfs/guoyiqiu/coding/huggingface/datasets/allenai___sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815 (last modified on Fri Mar  8 03:48:47 2024).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /mnt/petrelfs/guoyiqiu/coding/huggingface/datasets/allenai___sciq/default/0.0.0/2c94ad3e1aafab77146f384e23536f97a4849815 (last modified on Fri Mar  8 03:48:47 2024).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2024-03-08T11:19:15+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6043 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.157:2392->223.5.5.5:53: i/o timeout\"\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"allenai/sciq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572d25637bf331a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Inplementation of internal state based uncertainty estimation  \n",
    "- 使用一个统一的prompt，获取模型对训练集中的数据的预测结果  \n",
    "\t- prompt template  \n",
    "- 选取一个RougeL阈值，将模型的预测结果分为正确和错误两类，计算正确率  \n",
    "    - RougeL threshold\n",
    "- 对正确和错误的预测结果，分别记录其内部状态，计算certainty vector \n",
    "    - pair selection: \n",
    "        - 随机选择等量的正确和错误的题目，使用模型生成的回复组成pair (可能有噪，即多生成了一些无关内容)\n",
    "        - 随机选择等量的正确和错误的题目，使用标准答案和错误答案组成pair \n",
    "        - 选择模型生成后做对的题目，使用模型生成正确/错误答案时的句子组成pair\n",
    "\t- layer:\n",
    "\t    - 选择所有层的activation\n",
    "\t- token index\n",
    "\t    - 选择句子的最后一个token\n",
    "\t- activation\n",
    "\t    - 选择resid_post\n",
    "\t- clustering \n",
    "\t    - PCA\n",
    "- 使用相同的模板，获取模型在验证集上的预测结果，记录模型的内部状态  \n",
    "- 使用验证集的预测结果和模型的内部状态，计算uncertainty score  \n",
    "\t- token idx\n",
    "\t    - 选择句子的最后一个token\n",
    "- 使用uncertainty score和用RougeL计算出的模型的模型是否正确的标签，计算模型的AUROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3bc18de964e2d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:26:15.926954Z",
     "start_time": "2024-03-08T03:19:15.286227Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5178a64cd004e22a58db406537dab15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice science exam questions about Physics, Chemistry and Biology.\n",
      " Question:What type of organism is commonly used in preparation of foods such as cheese and yogurt?\n",
      " Options:viruses, gymnosperms, mesophilic organisms, protozoa\n",
      " Answer:Bacteria\n",
      "\n",
      "Question\n",
      "Test Accuracy: 66.20%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a144cf77e0824773a86643cd628a19b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are multiple choice science exam questions about Physics, Chemistry and Biology.\n",
      " Question:Who proposed the theory of evolution by natural selection?\n",
      " Options:Scopes, shaw, darwin, Linnaeus\n",
      " Answer:Charles Dar\n",
      "Test Accuracy: 67.70%\n"
     ]
    }
   ],
   "source": [
    "PROMPT_TEMPLATE = \"Question:{q} Options:{o} Answer:\"\n",
    "TRAIN_SIZE = 1000\n",
    "TEST_SIZE = 1000\n",
    "ROUGE_L_THRESHOLD = 0.5\n",
    "\n",
    "def get_dst_result(dst):\n",
    "    results = []\n",
    "    size = dst.num_rows\n",
    "    for i in tqdm(range(size)):\n",
    "        random.seed(i+42)\n",
    "        options = [dst[i][\"distractor1\"], dst[i][\"distractor2\"], dst[i][\"distractor3\"], dst[i][\"correct_answer\"]]\n",
    "        options_len = [len(model.to_str_tokens(opt, prepend_bos=False)) for opt in options]\n",
    "        max_new_tokens = max(options_len)\n",
    "        random.shuffle(options)\n",
    "        prompt = PROMPT_TEMPLATE.format(q=dst[i][\"question\"], o=\", \".join(options))\n",
    "        output = model.generate(prompt, max_new_tokens=max_new_tokens, do_sample=False,verbose=False)\n",
    "        answer = output.split(\"Answer:\")[1].strip()\n",
    "        print(output) if i == 0 else None\n",
    "        results.append(dict(\n",
    "            question=dst[i][\"question\"],\n",
    "            options=options,\n",
    "            answer=answer,\n",
    "            correct=dst[i][\"correct_answer\"],\n",
    "            input=prompt,\n",
    "            output=output\n",
    "        ))\n",
    "    for res in results:\n",
    "        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "        scores = scorer.score(res['answer'].lower(), res['correct'].lower())\n",
    "        res[\"score\"] = scores['rougeL']\n",
    "    \n",
    "    accuracy = sum([1 for res in results if res[\"score\"].fmeasure > ROUGE_L_THRESHOLD])/len(results)\n",
    "    print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "    return results\n",
    "\n",
    "train_dst = dataset[\"train\"].select(range(TRAIN_SIZE))\n",
    "train_result = get_dst_result(train_dst)\n",
    "\n",
    "val_dst = dataset[\"validation\"].select(range(TEST_SIZE))\n",
    "val_result = get_dst_result(val_dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5950794164644e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:26:15.937519Z",
     "start_time": "2024-03-08T03:26:15.928358Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CACHED_TOKEN_IDXS = -1\n",
    "CACHED_ACT_NAME = 'resid_post'\n",
    "CACHED_LAYERS = (0, model.cfg.n_layers)\n",
    "CORRECT_PROMPT_TEMPLATE = \"Question:{q}\\n Options:{o}\\n The correct answer is:\"\n",
    "INCORRECT_PROMPT_TEMPLATE = \"Question:{q}\\n Options:{o}\\n The incorrect answer is:\"\n",
    "\n",
    "\n",
    "def get_cache(res_list, layers, token_idx):\n",
    "    bsz = 16\n",
    "    batched_correct_res = [res_list[i:i+bsz] for i in range(0, len(res_list), bsz)]\n",
    "    all_cache = []\n",
    "    for batched_res in tqdm(batched_correct_res):\n",
    "        batched_input = [res['output'] for res in batched_res]\n",
    "        out, cache = model.run_with_cache(batched_input, names_filter=lambda x: CACHED_ACT_NAME in x, device='cpu')\n",
    "        batched_layer_cache = []\n",
    "        for l in range(model.cfg.n_layers):\n",
    "            batched_layer_cache.append(cache[utils.get_act_name(CACHED_ACT_NAME, l)].unsqueeze(0))\n",
    "        batched_layer_cache = torch.cat(batched_layer_cache, dim=0).transpose(0,1) # [bsz layer pos d_model]\n",
    "        all_cache.append(batched_layer_cache[:,layers[0]:layers[1],token_idx:,:])\n",
    "    all_cache = torch.cat(all_cache, dim=0) # [bsz layer pos d_model]\n",
    "    all_cache = all_cache # [bsz layer pos d_model]\n",
    "    return all_cache.float()\n",
    "\n",
    "def get_vector(res_list, layers, token_idx):\n",
    "    correct_res = [res for res in deepcopy(res_list) if res[\"score\"].fmeasure > ROUGE_L_THRESHOLD]\n",
    "    for res in correct_res:\n",
    "        # res['output'] = res['input'] + \" \" + res['correct']\n",
    "        res['output'] = CORRECT_PROMPT_TEMPLATE.format(q=res['question'], o=\", \".join(res['options'])) + \" \" + res['correct']\n",
    "    print(f\"Correct output: \\n{correct_res[0]['output']}\\n\\nGT: {correct_res[0]['correct']}\\n\\n\")\n",
    "    \n",
    "    incorrect_res = [res for res in deepcopy(res_list) if res[\"score\"].fmeasure > ROUGE_L_THRESHOLD]\n",
    "    for res in incorrect_res:\n",
    "        wrong_options = [opt for opt in res['options'] if opt != res['correct']]\n",
    "        # res['output'] = res['input'] + \" \" + random.choice(wrong_options)\n",
    "        res['output'] = INCORRECT_PROMPT_TEMPLATE.format(q=res['question'], o=\", \".join(res['options'])) + \" \" + random.choice(wrong_options)\n",
    "        \n",
    "    print(f\"Incorrect output: \\n{incorrect_res[0]['output']}\\n\\nGT: {incorrect_res[0]['correct']}\\n\\n\")\n",
    "    \n",
    "    correct_cache = get_cache(correct_res, layers, token_idx)\n",
    "    incorrect_cache = get_cache(incorrect_res, layers, token_idx)\n",
    "    print(f\"Correct Cache Shape: {correct_cache.shape}\")\n",
    "    print(f\"Incorrect Cache Shape: {incorrect_cache.shape}\")\n",
    "    \n",
    "    num_pair = min(correct_cache.shape[1], incorrect_cache.shape[0])\n",
    "    diff_vector = correct_cache[:num_pair] - incorrect_cache[:num_pair] # [b l d]\n",
    "    diff_vector = einops.rearrange(diff_vector, 'b l p d -> b (l p d)').cpu().float()\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(diff_vector)\n",
    "    certainty_vector = torch.tensor(pca.components_[0], dtype=torch.float) # [layer d_model]\n",
    "    certainty_vector = einops.rearrange(certainty_vector, '(l p d) -> l p d', p=abs(CACHED_TOKEN_IDXS), d=model.cfg.d_model) # [layer pos d_model]\n",
    "    print(f\"Certainty Vector Shape: {certainty_vector.shape}\")\n",
    "    return certainty_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75ee5414b454c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:26:55.989367Z",
     "start_time": "2024-03-08T03:26:15.938469Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct output: \n",
      "The following are multiple choice science exam questions about Physics, Chemistry and Biology.\n",
      " Question:What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
      " Options:tropical effect, coriolis effect, centrifugal effect, muon effect\n",
      " The correct answer is: coriolis effect\n",
      "\n",
      "GT: coriolis effect\n",
      "\n",
      "\n",
      "Incorrect output: \n",
      "The following are multiple choice science exam questions about Physics, Chemistry and Biology.\n",
      " Question:What phenomenon makes global winds blow northeast to southwest or the reverse in the northern hemisphere and northwest to southeast or the reverse in the southern hemisphere?\n",
      " Options:tropical effect, coriolis effect, centrifugal effect, muon effect\n",
      " The incorrect answer is: centrifugal effect\n",
      "\n",
      "GT: coriolis effect\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fbe80b45aa47369f9c9a08e41c1443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "141f2c0b57914f83acf60a6fe59ec3d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Cache Shape: torch.Size([662, 32, 1, 4096])\n",
      "Incorrect Cache Shape: torch.Size([662, 32, 1, 4096])\n",
      "Certainty Vector Shape: torch.Size([32, 1, 4096])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fabfcdcfa54a446eb0eda9d487aa3826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Cache Shape: torch.Size([1000, 32, 1, 4096])\n",
      "AUROC: 0.7255\n"
     ]
    }
   ],
   "source": [
    "def get_score_ours(val_result):\n",
    "    certainty_vector = get_vector(train_result, CACHED_LAYERS, CACHED_TOKEN_IDXS)\n",
    "    val_cache = get_cache(val_result, CACHED_LAYERS, CACHED_TOKEN_IDXS)\n",
    "    print(f\"Val Cache Shape: {val_cache.shape}\")\n",
    "    val_score = (einsum('b l p d, l p d -> b', val_cache, certainty_vector))\n",
    "    val_score = val_score.tolist()\n",
    "    # for res, score in zip(val_result, val_score):\n",
    "    #     print(f\"Question: {res['question']}\\nOptions: {res['options']}\\nAnswer: {res['answer']} \\nCorrect answer: {res['correct']}\\nRougeL: {res['score']}\\nUncertainty Score: {score:.4f}\\n\")\n",
    "\n",
    "    label = [1 if res[\"score\"].fmeasure > ROUGE_L_THRESHOLD else 0 for res in val_result]\n",
    "\n",
    "    auroc_score = roc_auc_score(label, val_score)\n",
    "    print(f'AUROC: {auroc_score if auroc_score > 0.5 else 1 - auroc_score:.4f}')\n",
    "    return auroc_score\n",
    "\n",
    "ours_auroc_score = get_score_ours(val_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e91e3b6456d644",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:27:38.351670Z",
     "start_time": "2024-03-08T03:26:55.991640Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2024-03-08T11:27:01+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6247 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: Post \\\"https://rubyfish.cn/dns-query\\\": context deadline exceeded\"\n",
      "time=\"2024-03-08T11:27:06+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6249 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.157:14053->223.5.5.5:53: i/o timeout\"\n",
      "time=\"2024-03-08T11:27:11+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6255 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.157:36910->223.5.5.5:53: i/o timeout\"\n",
      "time=\"2024-03-08T11:27:16+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6257 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.157:44304->119.29.29.29:53: i/o timeout\"\n",
      "time=\"2024-03-08T11:27:21+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6261 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.157:5131->119.29.29.29:53: i/o timeout\"\n",
      "time=\"2024-03-08T11:27:26+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6263 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.157:38422->119.29.29.29:53: i/o timeout\"\n",
      "time=\"2024-03-08T11:27:32+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6329 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: Post \\\"https://doh.pub/dns-query\\\": context deadline exceeded\"\n",
      "time=\"2024-03-08T11:27:38+08:00\" level=warning msg=\"[TCP] dial 选择节点 (match Match/) [::1]:6331 --> huggingface.co:443 error: forward1.10242018.xyz:6000 connect error: all DNS requests failed, first error: read udp4 10.140.0.157:39641->223.5.5.5:53: i/o timeout\"\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "bert_model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3409bd2b6d11b77e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-08T03:28:17.513191Z",
     "start_time": "2024-03-08T03:27:38.353150Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Result[0]: The following are multiple choice science exam questions about Physics, Chemistry and Biology.\n",
      " Question:Who proposed the theory of evolution by natural selection?\n",
      " Options:Scopes, shaw, darwin, Linnaeus\n",
      " Answer:Charles Dar\n",
      "Tokens Generated: ['Char', 'les', 'Dar']\n",
      "answer_idxs[0]:(50, 53)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756c15b63a2f4cc7baee99dcec11b7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all_sar_score[0]: 0.25166264176368713 all_pe_score[0]: 0.251953125 weights[0]: [0.3325665295124054, 0.3303814232349396, 0.3370520770549774]\n",
      "SAR AUROC: 0.6653\n",
      "PE AUROC: 0.6655\n"
     ]
    }
   ],
   "source": [
    "def get_score_sar(val_result):\n",
    "    label = [1 if res[\"score\"].fmeasure > ROUGE_L_THRESHOLD else 0 for res in val_result]\n",
    "    answer_idxs = []\n",
    "    for i,res in enumerate(val_result):\n",
    "        start = model.get_token_position('Answer', res['output']) + 2\n",
    "        end = len(model.to_str_tokens(res['output']))\n",
    "        answer_idxs.append((start, end))\n",
    "        print(f\"Val Result[0]: {res['output']}\\nTokens Generated: {model.to_str_tokens(res['output'])[start:end]}\") if i == 0 else None\n",
    "    print(f\"answer_idxs[0]:{answer_idxs[0]}\")\n",
    "    bsz = 16\n",
    "    batched_correct_res = [val_result[i:i+bsz] for i in range(0, len(val_result), bsz)]\n",
    "    batched_answer_idxs = [answer_idxs[i:i+bsz] for i in range(0, len(answer_idxs), bsz)]\n",
    "    all_sar_score = []\n",
    "    all_sar_weights = []\n",
    "    all_pe_score = []\n",
    "\n",
    "    for batched_res, batched_idxs in tqdm(zip(batched_correct_res, batched_answer_idxs), total=len(batched_correct_res)):\n",
    "        batched_input = [res['output'] for res in batched_res]\n",
    "        logits = model(batched_input, return_type='logits', padding_side='right') # [bsz seq vocab]\n",
    "        batched_orig_embedding = bert_model.encode(batched_input, convert_to_tensor=True)\n",
    "        for i in range(len(batched_res)):\n",
    "            input_string = batched_input[i]\n",
    "            input_str_tokens = model.to_str_tokens(input_string)\n",
    "            input_tokens = model.to_tokens(input_string)[0].tolist()\n",
    "            assert len(input_str_tokens) == len(input_tokens)\n",
    "            start, end = batched_idxs[i]\n",
    "            prob = F.softmax(logits[i], dim=-1)\n",
    "            # print(prob, prob.shape) if i == 0 and len(all_pe_score) == 0 else None\n",
    "            logp = -torch.log(prob) # [len vocab]\n",
    "            # print(prob[start:end].max(dim=-1)) if i == 0 and len(all_pe_score) == 0 else None\n",
    "            pe = torch.tensor([logp[k-1][input_tokens[k]] for j,k in enumerate(range(start, end))], dtype=torch.float)\n",
    "            # print(f'pe[0]: {pe.tolist()}') if i == 0 and len(all_pe_score) == 0 else None\n",
    "            \n",
    "            orig_embedding = batched_orig_embedding[i]\n",
    "            new_input_strings = []\n",
    "            for j in range(start, end):\n",
    "                new_input_tokens = input_tokens[:j] + input_tokens[j+1:]\n",
    "                new_input_string = model.to_string(new_input_tokens)\n",
    "                # print(new_input_string) if j == start else None\n",
    "                new_input_strings.append(new_input_string)\n",
    "            new_input_strings_embedding = bert_model.encode(new_input_strings, convert_to_tensor=True)\n",
    "            sim = util.cos_sim(orig_embedding, new_input_strings_embedding)[0].cpu()\n",
    "            weights = 1 - sim\n",
    "            weights = F.softmax(weights, dim=0)\n",
    "            sar_score = einsum('s, s ->', pe, weights).item()\n",
    "            pe_score = pe.mean().item()\n",
    "            all_sar_score.append(sar_score)\n",
    "            all_sar_weights.append(weights.tolist())\n",
    "            all_pe_score.append(pe_score)\n",
    "        # break\n",
    "    print(f\"all_sar_score[0]: {all_sar_score[0]} all_pe_score[0]: {all_pe_score[0]} weights[0]: {all_sar_weights[0]}\")\n",
    "    sar_auroc_score = roc_auc_score(label, all_sar_score)\n",
    "    pe_auroc_score = roc_auc_score(label, all_pe_score)\n",
    "    print(f'SAR AUROC: {sar_auroc_score if sar_auroc_score > 0.5 else 1 - sar_auroc_score:.4f}')\n",
    "    print(f'PE AUROC: {pe_auroc_score if pe_auroc_score > 0.5 else 1 - pe_auroc_score:.4f}')\n",
    "    return sar_auroc_score\n",
    "\n",
    "sar_auroc_score = get_score_sar(val_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
