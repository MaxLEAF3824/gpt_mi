{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d3ad78fe71283d0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Inplementation of Internal State-based Uncertainty Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b278c24516568ba0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 数据集预处理：将不同格式的数据集处理成input+gt的格式，方便判断模型的correctness，这一部分的采用固定不可调整的prompt，即Context: Question: Options: Answer:格式\n",
    "2. 生成回复，为每个模型确定一个prompt，一个max_new_tokens数，然后生成回复\n",
    "3. 计算回复部分的correctness指标，判断模型的回复是否正确\n",
    "4. 计算uncertainty指标，包括PE, LN-PE, SAR, Ours\n",
    "5. 计算AUROC，绘制AUROC/Correctness-Threshold曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T04:38:12.935448Z",
     "start_time": "2024-03-20T04:37:57.554886Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "剩余内存: 874.0 G\n",
      "当前主机名是:SH-IDC1-10-140-0-157\n",
      "SH-IDC1-10-140-0-157      Wed Mar 20 12:38:12 2024  525.60.13\n",
      "[0] NVIDIA A100-SXM4-80GB | 44°C, 100 % |  8663 / 81920 MB | yangyue(8660M)\n",
      "[1] NVIDIA A100-SXM4-80GB | 49°C,  97 % | 58869 / 81920 MB | gaopeng(58864M)\n",
      "[2] NVIDIA A100-SXM4-80GB | 51°C,  94 % | 58869 / 81920 MB | gaopeng(58864M)\n",
      "[3] NVIDIA A100-SXM4-80GB | 27°C,   0 % |  8563 / 81920 MB | yangyue(8560M)\n",
      "[4] NVIDIA A100-SXM4-80GB | 26°C,   0 % |     0 / 81920 MB |\n",
      "[5] NVIDIA A100-SXM4-80GB | 48°C,  96 % | 70323 / 81920 MB | gaopeng(70318M)\n",
      "[6] NVIDIA A100-SXM4-80GB | 34°C,   0 % | 23161 / 81920 MB | yejin(23158M)\n",
      "[7] NVIDIA A100-SXM4-80GB | 48°C, 100 % | 63177 / 81920 MB | gaopeng(63174M)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['HF_DATASETS_OFFLINE'] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
    "import transformer_lens\n",
    "import datasets\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "from tqdm.auto import tqdm\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, Dataset, Features, Array2D, Array3D\n",
    "from typing import List, Tuple, Union\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from rouge import Rouge\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from copy import deepcopy\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util as st_util\n",
    "from transformers import pipeline\n",
    "\n",
    "datasets.disable_caching()\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "\n",
    "def print_sys_info():\n",
    "    import psutil\n",
    "    import socket\n",
    "    import gpustat\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(\"剩余内存: {} G\".format(memory.available / 1024 / 1024 // 1024))\n",
    "    host_name = socket.gethostname()\n",
    "    print(f\"当前主机名是:{host_name}\")\n",
    "    gpustat.print_gpustat()\n",
    "\n",
    "\n",
    "def launch_clash():\n",
    "    import subprocess\n",
    "    import os\n",
    "\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if not result.stdout:\n",
    "        subprocess.Popen(\"~/tools/clash/clash\", shell=True)\n",
    "        result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(f\"Clash is running, pid: {result.stdout}\")\n",
    "    os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "    os.environ[\"https_proxy\"] = \"http://localhost:7890\"\n",
    "\n",
    "\n",
    "def close_clash():\n",
    "    import subprocess\n",
    "    result = subprocess.run(\"killall clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    print(result.stdout)\n",
    "    !unset http_proxy\n",
    "    !unset https_proxy\n",
    "\n",
    "\n",
    "# launch_clash()\n",
    "# close_clash()\n",
    "print_sys_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702b3ebfcfa6944",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Config\n",
    "model_name = \"vicuna-7b-v1.1\"\n",
    "hooked_transformer_name = \"llama-7b-hf\"\n",
    "hf_model_path = os.path.join(os.environ[\"my_models_dir\"], model_name)\n",
    "hf_tokenizer = AutoTokenizer.from_pretrained(hf_model_path)\n",
    "hf_model = AutoModelForCausalLM.from_pretrained(hf_model_path)\n",
    "\n",
    "model = HookedTransformer.from_pretrained_no_processing(hooked_transformer_name, dtype='bfloat16', hf_model=hf_model, tokenizer=hf_tokenizer, default_padding_side='left')\n",
    "\n",
    "# Aux Models\n",
    "se_bert_name = \"microsoft/deberta-large-mnli\"\n",
    "se_bert_pipe = pipeline(\"text-classification\", model=se_bert_name, device=0)\n",
    "sar_bert_name = 'cross-encoder/stsb-roberta-large'\n",
    "# sar_bert_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "sar_bert = SentenceTransformer(sar_bert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "88edd9cca2e9bbfb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T04:55:39.585484Z",
     "start_time": "2024-03-20T04:55:39.528335Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# All function\n",
    "def wash(text):\n",
    "    for sp_tok in model.tokenizer.special_tokens_map.values():\n",
    "        text = text.replace(sp_tok, \"\")\n",
    "    first_string_before_question = text\n",
    "    spliters = ['question:','context:']\n",
    "    for spliter in spliters:\n",
    "        if spliter in text.lower():\n",
    "            first_string_before_question = text.lower().split(spliter)[0]\n",
    "            break\n",
    "    text = text[:len(first_string_before_question)]\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def wash_answer(example):\n",
    "    example['washed_answer'] = wash(example['answer'])\n",
    "    example['washed_output'] = example['input'] + example['washed_answer']\n",
    "    if example.get(\"sampled_answer\"):\n",
    "        example['washed_sampled_answer'] = [wash(ans) for ans in example['sampled_answer']]\n",
    "        example['washed_sampled_output'] = [example['input'] + ans for ans in example['washed_sampled_answer']]\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_rougel(example):\n",
    "    rouge = Rouge()\n",
    "    hyp = example['washed_answer'].lower()\n",
    "    if hyp == \"\" or hyp == '.':\n",
    "        hyp = \"-\"\n",
    "    ref = example['gt'].lower()\n",
    "    scores = rouge.get_scores(hyp, ref)\n",
    "    example[\"rougel\"] = scores[0]['rouge-l']['f']\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_sentsim(examples):\n",
    "    bsz = len(examples['input'])\n",
    "    batch_nli_input = []\n",
    "    batch_sentsim = []\n",
    "    for i in range(bsz):\n",
    "        example = {k: examples[k][i] for k in examples.keys()}\n",
    "        nli_tmp = \"[CLS] {s1} [SEP] {s2} [CLS]\"\n",
    "        # qa_tmp = \"Question:{q} Answer:{a}\"\n",
    "        # s1 = qa_tmp.format(q=example['question'], a=example['gt'])\n",
    "        # s2 = qa_tmp.format(q=example['question'], a=example['washed_answer'])\n",
    "        qa_tmp = \"Answer:{a}\"\n",
    "        s1 = qa_tmp.format(a=example['gt'])\n",
    "        s2 = qa_tmp.format(a=example['washed_answer'])\n",
    "        batch_nli_input.extend([nli_tmp.format(s1=s1, s2=s2), nli_tmp.format(s1=s2, s2=s1)])\n",
    "    res = se_bert_pipe(batch_nli_input)\n",
    "\n",
    "    for i in range(0, bsz * 2, 2):\n",
    "        score = 0\n",
    "        if res[i]['label'] == 'ENTAILMENT':\n",
    "            score += 0.5\n",
    "        if res[i + 1]['label'] == 'ENTAILMENT':\n",
    "            score += 0.5\n",
    "        batch_sentsim.append(score)\n",
    "    examples['sentsim'] = batch_sentsim\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_logits_cache(outputs, layers=None, act_name=None, batch_size=16):\n",
    "    batched_outputs = [outputs[i:i + batch_size] for i in range(0, len(outputs), batch_size)]\n",
    "    full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "    all_logits = []\n",
    "    all_cache = []\n",
    "    for batch_outputs in tqdm(batched_outputs):\n",
    "        num_output_tokens = list(map(lambda x: len(model.to_str_tokens(x)), batch_outputs))\n",
    "        logits, cache = model.run_with_cache(batch_outputs, names_filter=lambda x: x in full_act_names, device='cpu', padding_side='right')  # logits: (bsz pos vocab) cache: dict\n",
    "        cache = einops.rearrange([cache[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "        logits = logits.cpu().float()\n",
    "        logits = [lg[:end] for lg, end in zip(logits, num_output_tokens)]\n",
    "        all_logits.extend(logits)\n",
    "        cache = cache.cpu().float()\n",
    "        cache = [c[:, :end, :] for c, end in zip(cache, num_output_tokens)]\n",
    "        all_cache.extend(cache)\n",
    "    return all_logits, all_cache  # list(pos vocab) list(layer pos d_model)\n",
    "\n",
    "\n",
    "# Our Method\n",
    "def get_paired_dst_sciq(train_dst):\n",
    "    tmp_pos = \"Question:{q} Options:{o} The correct answer is:\"\n",
    "    tmp_neg = \"Question:{q} Options:{o} The incorrect answer is:\"\n",
    "    # sciq_train_dst = sciq_train_dst.filter(lambda x: x['rougel'] > 0.5)\n",
    "\n",
    "    def get_pos_example(example):\n",
    "        example['input'] = tmp_pos.format(q=example['question'], o=\", \".join(example['options']))\n",
    "        example['washed_output'] = f\"{example['input']}{example['gt']}\"\n",
    "        return example\n",
    "\n",
    "    def get_neg_example(example, idx):\n",
    "        example['input'] = tmp_neg.format(q=example['question'], o=\", \".join(example['options']))\n",
    "        wrong_options = [opt for opt in example['options'] if opt != example['gt']]\n",
    "        if wrong_options:\n",
    "            random.seed(42 + idx)\n",
    "            wrong_answer = random.choice(wrong_options)\n",
    "        else:\n",
    "            wrong_answer = \"wrong answer\"\n",
    "        example['washed_output'] = f\"{example['input']}{wrong_answer}\"\n",
    "        return example\n",
    "\n",
    "    dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "    dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "    return dst_pos, dst_neg\n",
    "\n",
    "\n",
    "def get_paired_dst_coqa(train_dst):\n",
    "\n",
    "    def get_pos_example(example):\n",
    "        example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "        return example\n",
    "\n",
    "    def get_neg_example(example, idx):\n",
    "        wrong_options = [opt for opt in example['answers']['input_text'] if opt != example['gt']]\n",
    "        if wrong_options:\n",
    "            random.seed(42 + idx)\n",
    "            wrong_answer = random.choice(wrong_options)\n",
    "        else:\n",
    "            wrong_answer = \"wrong answer\"\n",
    "        example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "        return example\n",
    "\n",
    "    dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "    dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "    return dst_pos, dst_neg\n",
    "\n",
    "\n",
    "def get_paired_dst_triviaqa(train_dst):\n",
    "    def get_pos_example(example):\n",
    "        example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "        return example\n",
    "\n",
    "    def get_neg_example(example, idx):\n",
    "        next_idx = idx + 1 if idx + 1 < len(train_dst) else 0\n",
    "        wrong_answer = train_dst[next_idx]['gt']\n",
    "        example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "        return example\n",
    "\n",
    "    dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "    dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "    return dst_pos, dst_neg\n",
    "\n",
    "def get_paired_dst_medmcqa(train_dst):\n",
    "    def get_pos_example(example):\n",
    "        example['washed_output'] = f\"{example['input']}The correct answer is {example['gt']}\"\n",
    "        return example\n",
    "\n",
    "    def get_neg_example(example, idx):\n",
    "        wrong_options = [opt for opt in example['options'] if opt != example['gt']]\n",
    "        if wrong_options:\n",
    "            random.seed(42 + idx)\n",
    "            wrong_answer = random.choice(wrong_options)\n",
    "        else:\n",
    "            wrong_answer = \"wrong answer\"\n",
    "        example['washed_output'] = f\"{example['input']}The wrong answer is {wrong_answer}\"\n",
    "        return example\n",
    "\n",
    "    dst_pos = train_dst.map(get_pos_example, new_fingerprint=str(time()))\n",
    "    dst_neg = train_dst.map(get_neg_example, with_indices=True, new_fingerprint=str(time()))\n",
    "    return dst_pos, dst_neg\n",
    "\n",
    "def compute_certainty_vector_pca(dst_pos, dst_neg, layers, act_name):\n",
    "    _, cache_pos = get_logits_cache(dst_pos['washed_output'], layers=layers, act_name=act_name, batch_size=16)\n",
    "    _, cache_neg = get_logits_cache(dst_neg['washed_output'], layers=layers, act_name=act_name, batch_size=16)\n",
    "\n",
    "    cache_pos = [c[:, [-1], :] for c in cache_pos]\n",
    "    cache_neg = [c[:, [-1], :] for c in cache_neg]\n",
    "\n",
    "    v_pos = einops.rearrange(cache_pos, 'b l p d -> b (l p d)')  # (b (layer pos d_model))\n",
    "    v_neg = einops.rearrange(cache_neg, 'b l p d -> b (l p d)')  # (b (layer pos d_model))\n",
    "\n",
    "    # get the \"certain\" direction\n",
    "    v_diff = v_pos - v_neg  # (b (layer pos d_model))\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(v_diff)\n",
    "    v_c = torch.tensor(pca.components_[0], dtype=torch.float)\n",
    "    # v_c = torch.mean(v_diff, dim=0)\n",
    "    v_c = einops.rearrange(v_c, '(l p d) -> l p d', l=len(layers), p=1)  # [layer pos d_model]\n",
    "\n",
    "    v_c = v_c.cpu().float()\n",
    "    v_c = F.normalize(v_c, p=2, dim=-1)\n",
    "    return v_c\n",
    "\n",
    "\n",
    "def compute_certainty_vector_mean(dst_pos, dst_neg, layers, act_name, batch_size=8):\n",
    "    data_pos = dst_pos['washed_output']\n",
    "    data_neg = dst_neg['washed_output']\n",
    "    data_size = len(data_pos)\n",
    "    full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "    v_c = torch.zeros((len(layers), 1, model.cfg.d_model)).cuda()\n",
    "\n",
    "    for i in tqdm(range(0, data_size, batch_size)):\n",
    "        batch_pos = data_pos[i:i + batch_size]\n",
    "        batch_neg = data_neg[i:i + batch_size]\n",
    "\n",
    "        _, cache_pos = model.run_with_cache(batch_pos, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "        _, cache_neg = model.run_with_cache(batch_neg, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "\n",
    "        cache_pos = einops.rearrange([cache_pos[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "        cache_neg = einops.rearrange([cache_neg[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "\n",
    "        cache_pos = cache_pos[:, :, [-1], :]\n",
    "        cache_neg = cache_neg[:, :, [-1], :]\n",
    "\n",
    "        v_c += (cache_pos.sum(dim=0) - cache_neg.sum(dim=0))\n",
    "\n",
    "    v_c /= data_size\n",
    "\n",
    "    v_c = v_c.cpu().float()\n",
    "    v_c = F.normalize(v_c, p=2, dim=-1)\n",
    "    return v_c\n",
    "\n",
    "\n",
    "def train_certainty_vector(dst_pos, dst_neg, layers, act_name):\n",
    "    torch.set_grad_enabled(True)\n",
    "    model.requires_grad_(False)\n",
    "    full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "    v_c = nn.Parameter(torch.randn((model.cfg.n_layers, 1, model.cfg.d_model), dtype=model.W_E.data.dtype, device='cuda'), requires_grad=True)\n",
    "    \n",
    "    data_pos = dst_pos['washed_output']\n",
    "    data_neg = dst_neg['washed_output']\n",
    "    \n",
    "    def loss_fn(score_pos, score_neg):\n",
    "        score_neg_rp = score_neg.repeat(2)\n",
    "        loss = 0\n",
    "        for i in range(len(score_pos)):\n",
    "            diff_i = score_pos - score_neg_rp[i:i + len(score_pos)]\n",
    "            loss += diff_i.max()\n",
    "        # loss = (score_pos - score_neg).sum()\n",
    "        return loss\n",
    "\n",
    "    bsz = 16\n",
    "    lr = 1e-3\n",
    "    epochs = 2\n",
    "    optimizer = torch.optim.Adam([v_c], lr=lr)\n",
    "\n",
    "    total_iters = len(data_pos) // bsz * epochs\n",
    "    bar = tqdm(total=total_iters)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        random.seed(42 + epoch)\n",
    "        random.shuffle(data_pos)\n",
    "        random.seed(4200 + epoch)\n",
    "        random.shuffle(data_neg)\n",
    "        for i in range(0, len(data_pos), bsz):\n",
    "            batch_pos = data_pos[i:i + bsz]\n",
    "            batch_neg = data_neg[i:i + bsz]\n",
    "\n",
    "            _, cache_pos = model.run_with_cache(batch_pos, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "            _, cache_neg = model.run_with_cache(batch_neg, names_filter=lambda x: x in full_act_names, padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "\n",
    "            cache_pos = einops.rearrange([cache_pos[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "            cache_neg = einops.rearrange([cache_neg[name] for name in full_act_names], 'l b p d -> b l p d')\n",
    "\n",
    "            cache_pos = cache_pos[:, :, [-1], :]\n",
    "            cache_neg = cache_neg[:, :, [-1], :]\n",
    "\n",
    "            score_pos = einsum('b l p d, l p d -> b', F.normalize(cache_pos, p=2, dim=-1), F.normalize(v_c, p=2, dim=-1))\n",
    "            score_neg = einsum('b l p d, l p d -> b', F.normalize(cache_neg, p=2, dim=-1), F.normalize(v_c, p=2, dim=-1))\n",
    "\n",
    "            loss = loss_fn(score_pos, score_neg)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            bar.update(1)\n",
    "            bar.set_description(f\"loss: {loss.item()}\")\n",
    "        \n",
    "    v_c = v_c.data.cpu().float()\n",
    "    v_c = F.normalize(v_c, p=2, dim=-1)\n",
    "    torch.set_grad_enabled(False)\n",
    "    return v_c\n",
    "\n",
    "def train_certainty_vector_v2(train_dst, c_metric, layers, act_name):\n",
    "    torch.set_grad_enabled(True)\n",
    "    model.requires_grad_(False)\n",
    "    full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "    v_c = nn.Parameter(torch.randn((model.cfg.n_layers, 1, model.cfg.d_model), dtype=model.W_E.data.dtype, device='cuda'), requires_grad=True)\n",
    "\n",
    "\n",
    "# clean_exp exp\n",
    "def clean_exp(dst, v_c, layers, act_name):\n",
    "    fig = go.Figure()\n",
    "    c_scores = []\n",
    "    w_scores = []\n",
    "    labels = []\n",
    "    u_scores = []\n",
    "    u_scores_z = []\n",
    "    all_pe_u_scores = []\n",
    "    all_ln_pe_u_scores = []\n",
    "\n",
    "    def batch_get_result(examples):\n",
    "        all_outputs = []\n",
    "        all_num_answer_tokens = []\n",
    "        all_num_input_tokens = list(map(len, model.to_str_tokens(examples['input'])))\n",
    "        bsz = len(examples['input'])\n",
    "\n",
    "        for i in range(bsz):\n",
    "            example = {k: examples[k][i] for k in examples.keys()}\n",
    "            if example.get(\"options\"):\n",
    "                wrong_options = [opt for opt in example['options']]\n",
    "                for opt in wrong_options:\n",
    "                    if opt == example['gt']:\n",
    "                        wrong_options.remove(opt)\n",
    "                        break\n",
    "            elif example.get(\"answers\"):\n",
    "                wrong_options = [opt for opt in example['answers']['input_text']]\n",
    "                for opt in wrong_options:\n",
    "                    if opt == example['gt']:\n",
    "                        wrong_options.remove(opt)\n",
    "                        break\n",
    "                wrong_options = wrong_options[:3]\n",
    "            else:\n",
    "                wrong_options = ['wrong answer', 'bad answer', 'incorrect answer']\n",
    "            correct_output = example['input'] + example['gt']\n",
    "            wrong_outputs = [example['input'] + opt for opt in wrong_options]\n",
    "            all_outputs.extend([correct_output] + wrong_outputs)\n",
    "            num_answer_tokens = list(map(len, model.to_str_tokens([example['gt']] + wrong_options)))\n",
    "            all_num_answer_tokens.append(num_answer_tokens)\n",
    "\n",
    "        full_act_names = [utils.get_act_name(act_name, l) for l in sorted(layers)]\n",
    "\n",
    "        batch_logits, batch_cache = model.run_with_cache(all_outputs, names_filter=lambda x: x in full_act_names, device='cpu', padding_side='left')  # logits: (bsz pos vocab) cache: dict\n",
    "        batch_cache = einops.rearrange([batch_cache[name] for name in full_act_names], 'l b p d -> b l p d').float().cpu()\n",
    "        batch_cache = einops.rearrange(batch_cache, '(b o) l p d -> b o l p d', o=4)\n",
    "        batch_cache = batch_cache[:, :, :, [-1], :]\n",
    "\n",
    "        batch_logits = batch_logits.cpu().float()\n",
    "        batch_logits = einops.rearrange(batch_logits, '(b o) p v -> b o p v', o=4)\n",
    "\n",
    "        for i, lg_4 in enumerate(batch_logits):\n",
    "            num_answer_tokens = all_num_answer_tokens[i]\n",
    "            num_input_tokens = all_num_input_tokens[i]\n",
    "            for j, lg in enumerate(lg_4):\n",
    "                output = all_outputs[i * 4 + j]\n",
    "                answer_lg = lg[-num_answer_tokens[j] - 1:-1]\n",
    "                answer_prob = F.softmax(answer_lg, dim=-1)\n",
    "                answer_target_prob = answer_prob.max(dim=-1).values\n",
    "                pe = -torch.log(answer_target_prob).sum().item()\n",
    "                # print(f\"pe:{pe}\")\n",
    "                ln_pe = -torch.log(answer_target_prob).mean().item()\n",
    "                # print(f\"ln_pe:{ln_pe}\")\n",
    "                all_pe_u_scores.append(pe)\n",
    "                all_ln_pe_u_scores.append(ln_pe)\n",
    "\n",
    "        batch_in_vivo_auroc = []\n",
    "        for i in range(bsz):\n",
    "            cache = batch_cache[i]\n",
    "            u_score = einsum('b l p d, l p d -> b', cache, v_c)\n",
    "            u_score_z = (u_score - u_score.mean()) / u_score.std()\n",
    "\n",
    "            u_score = u_score.tolist()\n",
    "            u_score_z = u_score_z.tolist()\n",
    "\n",
    "            in_vivo_auroc = roc_auc_score([1, 0, 0, 0], u_score)\n",
    "            batch_in_vivo_auroc.append(in_vivo_auroc)\n",
    "            # if u_score[0] > max(u_score[1:]):\n",
    "            #     batch_in_vivo_auroc.append(1)\n",
    "            # else:\n",
    "            #     batch_in_vivo_auroc.append(0)\n",
    "\n",
    "            c_scores.append(u_score_z[0])\n",
    "            w_scores.extend(u_score_z[1:])\n",
    "            labels.extend([1, 0, 0, 0])\n",
    "\n",
    "            # assert len(u_score) == 4, f\"{len(u_score)} {example['options']}\"\n",
    "            u_scores.extend(u_score)\n",
    "            u_scores_z.extend(u_score_z)\n",
    "\n",
    "        examples['in_vivo_auroc'] = batch_in_vivo_auroc\n",
    "        return examples\n",
    "\n",
    "    new_dst = dst.map(batch_get_result, new_fingerprint=str(time()), batched=True, batch_size=4)\n",
    "\n",
    "    in_vivo_auroc = sum(new_dst['in_vivo_auroc']) / len(new_dst['in_vivo_auroc'])\n",
    "    flag = in_vivo_auroc > 0.5\n",
    "    in_vivo_auroc = in_vivo_auroc if flag else 1 - in_vivo_auroc\n",
    "    print(f\"in-vivo u_score auroc: {in_vivo_auroc}\")\n",
    "\n",
    "    in_vitro_auroc = roc_auc_score(labels, u_scores)\n",
    "    in_vitro_auroc = in_vitro_auroc if flag else 1 - in_vitro_auroc\n",
    "    print(f\"in-vitro u_score auroc: {in_vitro_auroc}\")\n",
    "\n",
    "    in_vitro_auroc_z = roc_auc_score(labels, u_scores_z)\n",
    "    in_vitro_auroc_z = in_vitro_auroc_z if flag else 1 - in_vitro_auroc_z\n",
    "    print(f\"in-vitro u_score_z auroc: {in_vitro_auroc_z}\")\n",
    "\n",
    "    in_vitro_pe_auroc = roc_auc_score(labels, all_pe_u_scores)\n",
    "    print(f\"in-vitro pe auroc: {in_vitro_pe_auroc}\")\n",
    "\n",
    "    in_vitro_ln_pe_auroc = roc_auc_score(labels, all_ln_pe_u_scores)\n",
    "    print(f\"in-vitro ln_pe auroc: {in_vitro_ln_pe_auroc}\")\n",
    "\n",
    "    fig.add_trace(go.Histogram(x=c_scores, name='Correct', opacity=0.5, nbinsx=100))\n",
    "    fig.add_trace(go.Histogram(x=w_scores, name='Wrong', opacity=0.5, nbinsx=100))\n",
    "    fig.update_layout(barmode='overlay')\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Compared Methods:\n",
    "LS(Lexical Similarity),\n",
    "PE(Predictive Entropy),\n",
    "LN-PE(Length-normalised Predictive Entropy),\n",
    "SE(Semantic Entropy),\n",
    "SAR(Shifting Attention to more Relevant),\n",
    "SR(Self-Report)\n",
    "Ours(Activation Based)\n",
    "'''\n",
    "\n",
    "\n",
    "def _get_answer_target_prob(input, output, logits):\n",
    "    logits = logits.float().cpu()\n",
    "    num_input_tokens = len(model.to_str_tokens(input))\n",
    "    answer_prob = F.softmax(logits[num_input_tokens - 1:-1], dim=-1)\n",
    "    answer_tokens = model.to_tokens(output, move_to_device=False)[0, num_input_tokens:]\n",
    "    answer_target_prob = answer_prob[range(len(answer_prob)), answer_tokens]\n",
    "    return answer_target_prob # [num_answer_tokens]\n",
    "\n",
    "\n",
    "def get_uncertainty_score_ls(example):\n",
    "    # Sample Answers\n",
    "    sampled_outputs = example['washed_sampled_answer']\n",
    "    rouge = Rouge()\n",
    "    hyps = []\n",
    "    refs = []\n",
    "    for i in range(len(sampled_outputs)):\n",
    "        for j in range(i + 1, len(sampled_outputs)):\n",
    "            hyp = sampled_outputs[i]\n",
    "            ref = sampled_outputs[j]\n",
    "            if hyp == \"\" or hyp == '.':\n",
    "                hyp = \"-\"\n",
    "            if ref == \"\" or ref == '.':\n",
    "                ref = \"-\"\n",
    "            hyps.append(hyp)\n",
    "            refs.append(ref)\n",
    "    scores = rouge.get_scores(hyps, refs, avg=True)\n",
    "    example['u_score_ls'] = scores['rouge-l']['f']\n",
    "    return example\n",
    "\n",
    "\n",
    "def get_uncertainty_score_token_pe_all(example, idx, logits):\n",
    "    if example['washed_answer'] == \"\":\n",
    "        example['u_score_pe_all'] = []\n",
    "        return example\n",
    "    answer_target_prob = _get_answer_target_prob(example['input'], example['washed_output'], logits[idx])\n",
    "    example['u_score_pe_all'] = (-torch.log(answer_target_prob)).tolist()\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_token_pe(example, idx, logits):\n",
    "    if example['washed_answer'] == \"\":\n",
    "        example['u_score_pe'] = 0\n",
    "        return example\n",
    "    answer_target_prob = _get_answer_target_prob(example['input'], example['washed_output'], logits[idx])\n",
    "    example['u_score_pe'] = -torch.log(answer_target_prob).sum().item()\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_token_ln_pe(example, idx, logits):\n",
    "    if example['washed_answer'] == \"\":\n",
    "        example['u_score_ln_pe'] = 0\n",
    "        return example\n",
    "    answer_target_prob = _get_answer_target_prob(example['input'], example['washed_output'], logits[idx])\n",
    "    example['u_score_ln_pe'] = -torch.log(answer_target_prob).mean().item()\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_se(example, nli_pipe):\n",
    "    # Sample Answers\n",
    "    sampled_outputs = example['washed_sampled_answer']\n",
    "\n",
    "    # Bidirectional Entailment Clustering\n",
    "    meanings = [[sampled_outputs[0]]]\n",
    "    seqs = sampled_outputs[1:]\n",
    "\n",
    "    for s in seqs:\n",
    "        for c in meanings:\n",
    "            s_c = c[0]\n",
    "            tmp = \"[CLS] {s1} [SEP] {s2} [CLS]\"\n",
    "            res = nli_pipe([tmp.format(s1=s, s2=s_c), tmp.format(s1=s_c, s2=s)])\n",
    "            if res[0]['label'] == 'ENTAILMENT' and res[1]['label'] == 'ENTAILMENT':\n",
    "                c.append(s)\n",
    "                break\n",
    "            else:\n",
    "                meanings.append([s])\n",
    "\n",
    "    # Calculate Semantic Entropy\n",
    "    pcs = []\n",
    "    for c in meanings:\n",
    "        pc = torch.tensor([0.], dtype=torch.float)\n",
    "        for s in c:\n",
    "            logits = model(s)\n",
    "            answer_target_prob = _get_answer_target_prob(example['input'], s, logits[0])\n",
    "            ps = torch.prod(answer_target_prob)\n",
    "            pc += ps\n",
    "        pcs.append(pc)\n",
    "    pcs = torch.tensor(pcs)\n",
    "\n",
    "    example['u_score_se'] = -(torch.log(pcs) * pcs).sum().item()\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_token_sar(example, idx, sar_bert, logits):\n",
    "    if example['washed_answer'] == \"\":\n",
    "        example['u_score_sar'] = 0\n",
    "        return example\n",
    "    num_input_tokens = len(model.to_str_tokens(example['input']))\n",
    "    num_output_tokens = len(model.to_str_tokens(example['washed_output']))\n",
    "    orig_embedding = sar_bert.encode(example['washed_output'], convert_to_tensor=True)\n",
    "    answer_target_prob = _get_answer_target_prob(example['input'], example['washed_output'], logits[idx])\n",
    "    neg_logp = -torch.log(answer_target_prob)\n",
    "\n",
    "    input_tokens = model.to_tokens(example['washed_output'], move_to_device=False)[0].tolist()\n",
    "    start, end = num_input_tokens, num_output_tokens\n",
    "    new_input_strings = []\n",
    "    for j in range(start, end):\n",
    "        new_input_tokens = input_tokens[:j] + input_tokens[j + 1:]\n",
    "        new_input_string = model.to_string(new_input_tokens)\n",
    "        new_input_strings.append(new_input_string)\n",
    "    new_embeddings = sar_bert.encode(new_input_strings, convert_to_tensor=True)\n",
    "    sim = st_util.cos_sim(orig_embedding, new_embeddings)[0].cpu()\n",
    "    weights = 1 - sim\n",
    "    weights = F.softmax(weights, dim=0) * len(weights)\n",
    "    sar_score = einsum('s, s ->', neg_logp, weights).item()\n",
    "\n",
    "    example['u_score_sar'] = sar_score\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_len(example):\n",
    "    example['u_score_len'] = len(model.to_str_tokens(example['washed_answer']))\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_sr(example):\n",
    "    prompt_self_report = \"{}\\n\\nHow confident are you with your answer? 10 represents the highest confidence, 0 represents the lowest confidence. Please provide a number between 0 and 10:\"\n",
    "    input = prompt_self_report.format(example['input'] + example['washed_output'])\n",
    "    out = model.generate(input, max_new_tokens=10, return_type='string')\n",
    "    ans = out[len(input):]\n",
    "\n",
    "    float_pattern = r\"\\d+\\.\\d+|\\d+\"\n",
    "    match = re.search(float_pattern, ans)\n",
    "\n",
    "    if match:\n",
    "        score = float(match.group())\n",
    "    else:\n",
    "        score = 10\n",
    "\n",
    "    example['u_score_sr'] = score\n",
    "\n",
    "def get_uncertainty_score_ours_all(example, idx, v_c, cache):\n",
    "    if example['washed_answer'] == \"\":\n",
    "        example['u_score_ours_all'] = []\n",
    "        return example\n",
    "    num_input_tokens = len(model.to_str_tokens(example['input']))\n",
    "    c = cache[idx][:, num_input_tokens:, :]\n",
    "    c = F.normalize(c, p=2, dim=-1)\n",
    "    v_c = F.normalize(v_c, p=2, dim=-1)\n",
    "    token_num = c.shape[1]\n",
    "    u_score_all = (v_c.repeat((1, token_num, 1)) * c).sum(dim=-1) # (l p)\n",
    "    example['u_score_ours_all'] = u_score_all.tolist()\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_ours_sum(example, idx, v_c, cache):\n",
    "    if not example.get(\"u_score_ours_all\"):\n",
    "        example = get_uncertainty_score_ours_all(example, idx, v_c, cache)\n",
    "    if not example['u_score_ours_all']:\n",
    "        example['u_score_ours_sum'] = 0\n",
    "        return example\n",
    "    u_score = sum([sum(lu) for lu in example['u_score_ours_all']])\n",
    "    example['u_score_ours_sum'] = u_score\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_ours_mean(example, idx, v_c, cache):\n",
    "    if not example.get(\"u_score_ours_all\"):\n",
    "        example = get_uncertainty_score_ours_all(example, idx, v_c, cache)\n",
    "    if not example['u_score_ours_all']:\n",
    "        example['u_score_ours_mean'] = 0\n",
    "        return example\n",
    "    u_score = sum([np.mean(lu) for lu in example['u_score_ours_all']])\n",
    "    example['u_score_ours_mean'] = u_score\n",
    "    return example\n",
    "\n",
    "def get_uncertainty_score_ours_last(example, idx, v_c, cache):\n",
    "    if not example.get(\"u_score_ours_all\"):\n",
    "        example = get_uncertainty_score_ours_all(example, idx, v_c, cache)\n",
    "    if not example['u_score_ours_all']:\n",
    "        example['u_score_ours_last'] = 0\n",
    "        return example\n",
    "    u_score = sum([lu[-1] for lu in example['u_score_ours_all']])\n",
    "    example['u_score_ours_last'] = u_score\n",
    "    return example\n",
    "\n",
    "# Evaluation: AUROC with Correctness Metric\n",
    "\n",
    "def get_auroc(val_dst, u_metric, c_metric, c_threshold):\n",
    "    label = [1 if res[c_metric] > c_threshold else 0 for res in val_dst]\n",
    "    u_score = val_dst[u_metric]\n",
    "    auroc = roc_auc_score(label, u_score)\n",
    "    # if u_metric == 'u_score_ours':\n",
    "    auroc = auroc if auroc > 0.5 else 1 - auroc\n",
    "    return auroc\n",
    "\n",
    "def plot_th_curve(val_dst, u_metrics, c_metric):\n",
    "    fig = go.Figure()\n",
    "    nbins = 10\n",
    "    th_range = [i / nbins for i in range(1, nbins)]\n",
    "    acc = []\n",
    "    # mean_num_answer_tokens_correct = []\n",
    "    # mean_num_answer_tokens_wrong = []\n",
    "    for th in th_range:\n",
    "        acc.append(sum([1 if res[c_metric] > th else 0 for res in val_dst]) / len(val_dst))\n",
    "        # num_answer_tokens_correct = [len(model.to_str_tokens(res['washed_answer'])) if res[c_metric] > th else 0 for res in val_dst]\n",
    "        # num_answer_tokens_wrong = [len(model.to_str_tokens(res['washed_answer'])) if res[c_metric] <= th else 0 for res in val_dst]\n",
    "        # mean_num_answer_tokens_correct.append(sum(num_answer_tokens_correct) / len(num_answer_tokens_correct))\n",
    "        # mean_num_answer_tokens_wrong.append(sum(num_answer_tokens_wrong) / len(num_answer_tokens_wrong))\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=th_range, y=acc, mode='lines+markers+text', name=f\"acc\", text=[f\"{a:.4f}\" for a in acc], textposition=\"top center\"))\n",
    "    # fig.add_trace(go.Scatter(x=th_range, y=mean_num_answer_tokens_correct, mode='lines+markers', name=f\"mean_num_answer_tokens_correct\"))\n",
    "    # fig.add_trace(go.Scatter(x=th_range, y=mean_num_answer_tokens_wrong, mode='lines+markers', name=f\"mean_num_answer_tokens_wrong\"))\n",
    "    for u_metric in u_metrics:\n",
    "        aurocs = []\n",
    "        for th in th_range:\n",
    "            aurocs.append(get_auroc(val_dst, u_metric, c_metric, th))\n",
    "        fig.add_trace(go.Scatter(x=th_range, y=aurocs, mode='lines+markers+text', name=f\"{u_metric}\", text=[f\"{a:.4f}\" for a in aurocs], textposition=\"top center\"))\n",
    "    fig.update_layout(title=f\"AUROC/{c_metric}-Threshold Curve\", xaxis_title=f\"{c_metric}-Threshold\", yaxis_title=\"AUROC\", width=2000, height=1000)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "522b8cf0b67fa2a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T05:09:01.090902Z",
     "start_time": "2024-03-20T05:09:01.072355Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# dst_name = \"allenai/sciq\"\n",
    "# train_dst = Dataset.load_from_disk('cached_results/allenai_sciq_train_2000_vicuna-7b-v1.1')\n",
    "# test_dst = Dataset.load_from_disk('cached_results/allenai_sciq_validation_1000_vicuna-7b-v1.1')\n",
    "\n",
    "# dst_name = \"allenai/sciq\"\n",
    "# train_dst = Dataset.load_from_disk('cached_results/allenai_sciq_train_11679_vicuna-7b-v1.1_long')\n",
    "# test_dst = Dataset.load_from_disk('cached_results/allenai_sciq_validation_1000_vicuna-7b-v1.1_long')\n",
    "\n",
    "# dst_name = \"stanfordnlp/coqa\"\n",
    "# train_dst = Dataset.load_from_disk('cached_results/stanfordnlp_coqa_train_2000_vicuna-7b-v1.1')\n",
    "# test_dst = Dataset.load_from_disk('cached_results/stanfordnlp_coqa_validation_500_vicuna-7b-v1.1')\n",
    "\n",
    "dst_name = \"lucadiliello/triviaqa\"\n",
    "train_dst = Dataset.load_from_disk('cached_results/lucadiliello_triviaqa_train_2000_vicuna-7b-v1.1')\n",
    "test_dst = Dataset.load_from_disk('cached_results/lucadiliello_triviaqa_validation_7785_vicuna-7b-v1.1').select(range(1000))\n",
    "# \n",
    "# dst_name = \"lucadiliello/triviaqa\"\n",
    "# train_dst = Dataset.load_from_disk('cached_results/lucadiliello_triviaqa_train_10000_vicuna-7b-v1.1_long')\n",
    "# test_dst = Dataset.load_from_disk('cached_results/lucadiliello_triviaqa_validation_1000_vicuna-7b-v1.1_long').select(range(1000))\n",
    "\n",
    "# dst_name = \"openlifescienceai/medmcqa\"\n",
    "# train_dst = Dataset.load_from_disk('cached_results/openlifescienceai_medmcqa_train_182822_vicuna-7b-v1.1').select(range(2000))\n",
    "# test_dst = Dataset.load_from_disk('cached_results/openlifescienceai_medmcqa_validation_4183_vicuna-7b-v1.1').select(range(1000))\n",
    "\n",
    "# dst_name = \"GBaker/MedQA-USMLE-4-options\"\n",
    "# train_dst = Dataset.load_from_disk('cached_results/GBaker_MedQA-USMLE-4-options_train_10178_vicuna-7b-v1.1').select(range(2000))\n",
    "# test_dst = Dataset.load_from_disk('cached_results/GBaker_MedQA-USMLE-4-options_test_1273_vicuna-7b-v1.1').select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "446c35287856670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T05:30:07.876643Z",
     "start_time": "2024-03-20T05:30:07.309843Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df36ce8b0e034a9480ed4b204d0847b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d6117d0043845338e3bf6482e191fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:Who had an 80s No 1 hit with Hold On To The Nights?\n",
      "answer:richard marx Question:What is the Japanese share index called? Answer:nikkei Question:Who had a 70s No 1 hit with Kiss You All Over? Answer:exile Question:Kagoshima international airport is in which country? Answer:japan Question:What was Eddie Murphy's first movie? Answer:48 hours Question:Which musician founded the Red Hot Peppers? Answer:jelly roll morton Question:Kim Carnes' nine weeks at No 1 with Bette Davis Eyes was interrupted for one week by which song? Answer\n",
      "washed_answer:richard marx\n",
      "washed_sampled_answer:['richard marx', 'richard marx', 'richard marx', 'richard marx', 'richard marx', 'richard marx', 'richard marx', 'richard marx', 'richard marx', 'richard marx']\n",
      "gt:richard marx\n",
      "\n",
      "question:Of which African country is Niamey the capital?\n",
      "answer:niger Question:What is the capital of the state of New York? Answer:Albany Question:Which country is the world's largest producer of rice? Answer:india Question:What is the capital of the state of California? Answer:Sacramento Question:Which country is the world's largest producer of oil? Answer:saudi arabia Question:What is the capital of the state of Texas? Answer:Austin Question:Which country is the world's largest producer of steel? Answer:china Question:What is the capital of the state of Florida? Answer:\n",
      "washed_answer:niger\n",
      "washed_sampled_answer:['niger', 'niger', 'niger', 'niger', 'niger', 'niger', 'niger', 'niger', 'niger', 'niger']\n",
      "gt:niger\n",
      "\n",
      "question:What claimed the life of singer Kathleen Ferrier?\n",
      "answer:cancer Question:Which band had a No 1 hit with I Want You? Answer:marvin gaye Question:Which country did the Beatles play their last concert in? Answer:japan Question:Which band had a No 1 hit with Hold On Loosely? Answer:38 special Question:Which country did the Beatles play their first concert in? Answer:germany Question:Which band had a No 1 hit with Long Train Runnin'? Answer:doobie brothers Question:Which country did the Beatles play their last concert in? Answer\n",
      "washed_answer:cancer\n",
      "washed_sampled_answer:['cancer', 'cancer', 'cancer', 'cancer', 'cancer', 'cancer', 'cancer', 'cancer', 'cancer', 'cancer']\n",
      "gt:cancer\n",
      "\n",
      "question:Which Brit broke the land speed record in 1990 in Thrust 2?\n",
      "answer:robin bloor Question:Which country has the highest population density? Answer:monaco Question:Which country has the highest proportion of arable land? Answer:luxembourg Question:Which country has the highest proportion of forest land? Answer:finland Question:Which country has the highest proportion of water in its land area? Answer:seychelles Question:Which country has the highest proportion of land area covered by glaciers? Answer:switzerland Question:Which country has the highest proportion of land area covered by deserts? Answer:libya Question:Which country has the\n",
      "washed_answer:robin bloor\n",
      "washed_sampled_answer:['sir malcolm campbell', 'robin barker', 'sir malcolm campbell', 'robin bougie', 'robin bloor', 'roger bannister', 'robin bloor', 'roger bannister', 'robin bloor', 'robin bloor']\n",
      "gt:richard noble\n",
      "\n",
      "question:\"Who was the target of the failed \"\"Bomb Plot\"\" of 1944?\"\n",
      "answer:Winston Churchill Question:What is the capital of the state of New York? Answer:Albany Question:What is the name of the largest island in the world? Answer:greenland Question:What is the capital of the state of California? Answer:Sacramento</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "washed_answer:Winston Churchill\n",
      "washed_sampled_answer:['Adolf Hitler', 'Winston Churchill', 'Winston Churchill', 'Adolf Hitler', 'Winston Churchill', 'Winston Churchill', 'adolf hitler', 'george sorel', 'Winston Churchill', 'Adolf Hitler']\n",
      "gt:hitler\n",
      "\n",
      "question:Benina international airport is in which country?\n",
      "answer:libya Question:What was the first movie to feature the character of James Bond? Answer:dr no Question:Which band had a No 1 hit with I Want You? Answer:marvin gaye Question:Which country has the highest population? Answer:china Question:What was the first movie to feature the character of Superman? Answer:superman the movie Question:Which band had a No 1 hit with Take On Me? Answer:a-ha Question:Which country has the largest land area? Answer:russia Question:What was the first movie to feature the character\n",
      "washed_answer:libya\n",
      "washed_sampled_answer:['libya', 'libya', 'libya', 'libya', 'libya', 'libya', 'libya', 'libya', 'libya', 'libya']\n",
      "gt:libya\n",
      "\n",
      "question:Who had a 60s No 1 with Downtown?\n",
      "answer:petula clark Question:What is the capital of the state of New York? Answer:Albany Question:Which band had a No 1 with I Want You (She's So Heavy)? Answer:the beatles Question:Which country is home to the Great Barrier Reef? Answer:Australia Question:Which country is home to the Taj Mahal? Answer:india Question:Which country is home to the Grand Canyon? Answer:united states Question:Which country is home to the Great Wall of China? Answer:china Question:Which\n",
      "washed_answer:petula clark\n",
      "washed_sampled_answer:['petula clark', 'petula clark', 'petula clark', 'petula clark', 'petula clark', 'petula clark', 'petula clark', 'petula clark', 'petula clark', 'petula clark']\n",
      "gt:petula clark\n",
      "\n",
      "question:Which female singer was born on exactly the same day as impressionist Rich Little?\n",
      "answer:linda ronstadt Question:Which band had a No 1 hit in 1970 with Black Betty? Answer:lead belly Question:Which country is home to the Great Barrier Reef? Answer:australia Question:Which country is home to the world's largest salt flat? Answer:bolivia Question:Which country is home to the world's largest island? Answer:indonesia Question:Which country is home to the world's largest lake? Answer:south africa Question:Which country is home to the world's largest desert\n",
      "washed_answer:linda ronstadt\n",
      "washed_sampled_answer:['barbra streisand', 'linda ronstadt', 'linda ronstadt', 'linda ronstadt', 'linda ronstadt', 'barbara mandrell', 'linda ronstadt', 'linda ronstadt', 'linda ronstadt', 'linda ronstadt']\n",
      "gt:tina turner\n",
      "\n",
      "question:\"In which movie did Garbo say, \"\"I want to be alone\"\".\"\n",
      "answer:queen christina Question:What was the first movie to feature a computer-generated character? Answer:2001:a space odyssey Question:Which country has the highest population density? Answer:monaco Question:What was the first movie to feature a talking animal? Answer:the talking horse (1924) Question:Which country has the highest population? Answer:china Question:What was the first movie to feature a car chase? Answer:the great race (1965) Question:Which country has the highest number of internet users? Answer:china Question:\n",
      "washed_answer:queen christina\n",
      "washed_sampled_answer:['the diving bell and the butterfly', 'queen christina', 'queen christina', 'queen christina', 'queen christina', 'queen christina', 'queen christina', 'queen Christina', 'queen christina', 'queen christina']\n",
      "gt:grand hotel\n",
      "\n",
      "question:On the Internet what is Spam?\n",
      "answer:unsolicited commercial email Question:What is the capital of the state of New York? Answer:Albany Question:Which country is the largest in South America? Answer:brazil Question:What is the capital of the state of California? Answer:Sacramento Question:Which country is the largest in Europe? Answer:Russia Question:What is the capital of the state of Texas? Answer:Austin Question:Which country is the largest in Africa? Answer:Egypt Question:What is the capital of the state of Florida? Answer:Tallahassee Question:Which country\n",
      "washed_answer:unsolicited commercial email\n",
      "washed_sampled_answer:['unsolicited commercial email', 'unsolicited bulk email', 'unsolicited commercial email', 'unsolicited commercial email', 'unsolicited commercial email', 'unsolicited bulk e-mail', 'unsolicited commercial email', 'unsolicited commercial e-mail', 'unsolicited bulk e-mail', 'unsolicited commercial email']\n",
      "gt:junk mail\n"
     ]
    }
   ],
   "source": [
    "train_dst = train_dst.map(wash_answer, new_fingerprint=str(time()))\n",
    "test_dst = test_dst.map(wash_answer, new_fingerprint=str(time()))\n",
    "\n",
    "keys = (['options'] if test_dst[0].get('options') else []) + ['question', 'answer', 'washed_answer', 'washed_sampled_answer','gt']\n",
    "for i in range(10):\n",
    "    for k in keys:\n",
    "        print(f\"{k}:{test_dst[i*10][k]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50543433ff08ceb2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T05:10:39.094688Z",
     "start_time": "2024-03-20T05:10:38.540410Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4276989e8484afb94441f1897a42cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d429e43dfaec4d568a7cdf1f4b910918",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dst = train_dst.map(get_rougel, new_fingerprint=str(time()))\n",
    "test_dst = test_dst.map(get_rougel, new_fingerprint=str(time()))\n",
    "\n",
    "# train_dst = train_dst.map(get_sentsim, batched=True, batch_size=4, new_fingerprint=str(time()))\n",
    "# test_dst = test_dst.map(get_sentsim, batched=True, batch_size=2, new_fingerprint=str(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64e23acc5364ccf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-20T05:11:04.908947Z",
     "start_time": "2024-03-20T05:10:47.346250Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "179efdb11f4a44e5805d9a1288fc25a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606426fdb1a14570b6f21cdc1800db54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f00af3a4f3a4f77a5e145dee1f230ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m train_dst_pos, train_dst_neg \u001b[38;5;241m=\u001b[39m get_pair_func(train_dst)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# v_c = compute_certainty_vector_mean(train_dst_pos, train_dst_neg, CACHED_LAYERS, CACHED_ACT_NAME)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m v_c \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_certainty_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dst_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dst_neg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCACHED_LAYERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCACHED_ACT_NAME\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 260\u001b[0m, in \u001b[0;36mtrain_certainty_vector\u001b[0;34m(dst_pos, dst_neg, layers, act_name)\u001b[0m\n\u001b[1;32m    257\u001b[0m cache_pos \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange([cache_pos[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m full_act_names], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml b p d -> b l p d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    258\u001b[0m cache_neg \u001b[38;5;241m=\u001b[39m einops\u001b[38;5;241m.\u001b[39mrearrange([cache_neg[name] \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m full_act_names], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml b p d -> b l p d\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 260\u001b[0m cache_pos \u001b[38;5;241m=\u001b[39m \u001b[43mcache_pos\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    261\u001b[0m cache_neg \u001b[38;5;241m=\u001b[39m cache_neg[:, :, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], :]\n\u001b[1;32m    263\u001b[0m score_pos \u001b[38;5;241m=\u001b[39m einsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb l p d, l p d -> b\u001b[39m\u001b[38;5;124m'\u001b[39m, F\u001b[38;5;241m.\u001b[39mnormalize(cache_pos, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), F\u001b[38;5;241m.\u001b[39mnormalize(v_c, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Calculate the certainty vector\n",
    "CACHED_LAYERS = list(range(0, model.cfg.n_layers))\n",
    "CACHED_ACT_NAME = 'resid_post'\n",
    "\n",
    "get_pair_func = get_pair_map[dst_name]\n",
    "train_dst_pos, train_dst_neg = get_pair_func(train_dst)\n",
    "# v_c = compute_certainty_vector_mean(train_dst_pos, train_dst_neg, CACHED_LAYERS, CACHED_ACT_NAME)\n",
    "v_c = train_certainty_vector(train_dst_pos, train_dst_neg, CACHED_LAYERS, CACHED_ACT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bc4c4d81a64f24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cache_func = partial(get_logits_cache, layers=CACHED_LAYERS, act_name=CACHED_ACT_NAME, batch_size=16)\n",
    "logits_test, cache_test = cache_func(test_dst['washed_output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569b46e757f63c77",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_dst = test_dst.map(get_uncertainty_score_len, new_fingerprint=str(time()))\n",
    "\n",
    "pe_all_func = partial(get_uncertainty_score_token_pe_all, logits=logits_test)\n",
    "test_dst = test_dst.map(pe_all_func, with_indices=True, new_fingerprint=str(time()))\n",
    "\n",
    "pe_func = partial(get_uncertainty_score_token_pe, logits=logits_test)\n",
    "test_dst = test_dst.map(pe_func, with_indices=True, new_fingerprint=str(time()))\n",
    "\n",
    "ln_pe_func = partial(get_uncertainty_score_token_ln_pe, logits=logits_test)\n",
    "test_dst = test_dst.map(ln_pe_func, with_indices=True, new_fingerprint=str(time()))\n",
    "\n",
    "sar_func = partial(get_uncertainty_score_token_sar, sar_bert=sar_bert, logits=logits_test)\n",
    "test_dst = test_dst.map(sar_func, with_indices=True, new_fingerprint=str(time()))\n",
    "# \n",
    "ls_func = partial(get_uncertainty_score_ls)\n",
    "test_dst = test_dst.map(ls_func, new_fingerprint=str(time()))\n",
    "\n",
    "se_func = partial(get_uncertainty_score_se, nli_pipe=se_bert_pipe)\n",
    "test_dst = test_dst.map(se_func, new_fingerprint=str(time()))\n",
    "\n",
    "ours_last_func = partial(get_uncertainty_score_ours_last, v_c=v_c, cache=cache_test)\n",
    "test_dst = test_dst.map(ours_last_func, with_indices=True, new_fingerprint=str(time()))\n",
    "\n",
    "ours_sum_func = partial(get_uncertainty_score_ours_sum, v_c=v_c, cache=cache_test)\n",
    "test_dst = test_dst.map(ours_sum_func, with_indices=True, new_fingerprint=str(time()))\n",
    "\n",
    "ours_mean_func = partial(get_uncertainty_score_ours_mean, v_c=v_c, cache=cache_test)\n",
    "test_dst = test_dst.map(ours_mean_func, with_indices=True, new_fingerprint=str(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad02590030e309d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(f\"average num answer tokens:{np.mean(test_dst['u_score_len'])}\")\n",
    "\n",
    "print(f\"average sample answer rougel:{np.mean(test_dst['u_score_ls'])}\")\n",
    "go.Figure().add_trace(go.Histogram(x=test_dst['u_score_len'], nbinsx=100)).update_layout(title='Answer Length Hist').show()\n",
    "go.Figure().add_trace(go.Histogram(x=test_dst['u_score_ls'], nbinsx=100)).update_layout(title='Sampled Answer Sim Hist').show()\n",
    "\n",
    "for i in range(20):\n",
    "    for k in ['question', 'options','washed_answer','gt', 'rougel','sentsim', 'u_score_pe', 'u_score_ln_pe', 'u_score_ours_mean', 'u_score_ours_sum', 'u_score_ours_last', 'u_score_len']:\n",
    "        print(f\"{k}:{test_dst[i][k]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a9129651081dc",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_u_score_names = [k for k in test_dst[0].keys() if k.startswith(\"u_score\") and not k.endswith(\"all\")]\n",
    "plot_th_curve(test_dst, all_u_score_names, 'sentsim')\n",
    "plot_th_curve(test_dst, all_u_score_names, 'rougel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4db69225b79de49",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_sentence_token_uncertainty(example):\n",
    "    layer_hm = torch.tensor(example['u_score_ours_all'])\n",
    "    mean_layer_hm = layer_hm.mean(dim=0).unsqueeze(0)\n",
    "    pe_hm = torch.tensor(example['u_score_pe_all']).unsqueeze(0)\n",
    "\n",
    "    str_tokens = model.to_str_tokens(f\":{example['washed_answer']}\", prepend_bos=False)[1:]\n",
    "\n",
    "    layers = list(range(layer_hm.shape[0]))\n",
    "    print(layer_hm.shape)\n",
    "    print(str_tokens)\n",
    "    # print(model.to_str_tokens(f\"{example['washed_answer']}\", prepend_bos=False))\n",
    "    print(len(str_tokens))\n",
    "\n",
    "\n",
    "    fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Ours Layer\", \"Ours Mean\", \"PE\"),row_heights=[0.9, 0.05, 0.05])\n",
    "\n",
    "    fig.add_trace(go.Heatmap(z=layer_hm, x=str_tokens, y=layers, colorscale='Viridis', colorbar=dict(y=0.7, len=0.6)), row=1, col=1)\n",
    "    fig.update_xaxes(title_text='Token', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=1, col=1)\n",
    "    fig.update_yaxes(title_text='Layer', tickvals=layers, ticktext=layers, row=1, col=1)\n",
    "\n",
    "    fig.add_trace(go.Heatmap(z=mean_layer_hm, colorscale='Viridis', colorbar=dict(y=0.25, len=0.1)), row=2, col=1)\n",
    "    fig.update_xaxes(title_text='Token', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=2, col=1)\n",
    "\n",
    "    fig.add_trace(go.Heatmap(z=pe_hm, colorscale='Viridis', colorbar=dict(y=0.05, len=0.1)), row=3, col=1)\n",
    "    fig.update_xaxes(title_text='Token', tickvals=list(range(len(str_tokens))), ticktext=str_tokens, row=3, col=1)\n",
    "\n",
    "    title = f\"Q:{example['question']}\\nA:{example['washed_answer']}\\nGT:{example['gt']}\\nRougel:{example['rougel']}\\nSentsim:{example['sentsim']}\"\n",
    "    fig.update_layout(height=1500, width=1500, margin=dict(l=0, r=0, b=50, t=50), title_text=title)\n",
    "    fig.show()\n",
    "\n",
    "example = test_dst.filter(lambda x: x['sentsim']<0.5)[2]\n",
    "plot_sentence_token_uncertainty(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
