{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unofficial Inplementation of In-Context-Vector with transformer_lens Library\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clash is running, pid: 9505\n",
      "\n",
      "--2024-02-23 10:45:47--  https://huggingface.co/\n",
      "Resolving localhost (localhost)... 127.0.0.1\n",
      "Connecting to localhost (localhost)|127.0.0.1|:7890... connected.\n",
      "Unable to establish SSL connection.\n"
     ]
    }
   ],
   "source": [
    "# launch clash\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if not result.stdout:\n",
    "    subprocess.Popen(\"clash\", shell=True)\n",
    "    result = subprocess.run(\"pidof clash\", shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "print(f\"Clash is running, pid: {result.stdout}\")\n",
    "os.environ[\"http_proxy\"] = \"http://localhost:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://localhost:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前主机名是：maxleaf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "838f1532c3c1420db0981c99077cd509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b-hf into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since s-nlp/paradetox couldn't be found on the Hugging Face Hub\n",
      "WARNING:datasets.load:Using the latest cached version of the dataset since s-nlp/paradetox couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/guoyiqiu/.cache/huggingface/datasets/s-nlp___paradetox/default/0.0.0/e4ebf0fa7a6705cb2c4e2e1426f1034f63bf6fa9 (last modified on Tue Feb 20 19:02:59 2024).\n",
      "WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'default' at /home/guoyiqiu/.cache/huggingface/datasets/s-nlp___paradetox/default/0.0.0/e4ebf0fa7a6705cb2c4e2e1426f1034f63bf6fa9 (last modified on Tue Feb 20 19:02:59 2024).\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "hostname = socket.gethostname()\n",
    "print(\"当前主机名是：\" + hostname)\n",
    "IS_CLUSTER = \"SH-IDC\" in hostname\n",
    "\n",
    "if IS_CLUSTER:\n",
    "    model_path = os.path.join(os.environ['my_models_dir'], 'llama-7b')\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = HookedTransformer.from_pretrained_no_processing(\"llama-7b-hf\", hf_model=hf_model, tokenizer=hf_tokenizer, dtype='float16', default_padding_side='left')\n",
    "else:\n",
    "    # model_path = \"google/gemma-2b\"\n",
    "    model_path = os.path.join(os.environ['my_models_dir'], 'llama-7b')\n",
    "    hf_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16, device_map = \"cuda:0\", load_in_4bit=True)\n",
    "    hf_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    model = HookedTransformer.from_pretrained_no_processing(\"llama-7b-hf\", hf_model=hf_model, tokenizer=hf_tokenizer, dtype='float16', default_padding_side='left')\n",
    "    # model = HookedTransformer.from_pretrained_no_processing(\"gpt2-xl\", dtype='float16', load_in_4bit=True, default_padding_side='left')\n",
    "dataset = load_dataset(\"s-nlp/paradetox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACT_NAME = 'resid_post'\n",
    "PROMPT=\"Instruction: Please paraphrase the following sentence.\\n\"\n",
    "FS_TEMPLATE = \"Sentence:{}\\nParaphrase:{}\"\n",
    "\n",
    "def svd_flip(u, v):\n",
    "    # columns of u, rows of v\n",
    "    max_abs_cols = torch.argmax(torch.abs(u), 0)\n",
    "    i = torch.arange(u.shape[1]).to(u.device)\n",
    "    signs = torch.sign(u[max_abs_cols, i])\n",
    "    u *= signs\n",
    "    v *= signs.view(-1, 1)\n",
    "    return u, v\n",
    "\n",
    "class PCA(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fit(self, X):\n",
    "        n, d = X.size()\n",
    "        if self.n_components is not None:\n",
    "            d = min(self.n_components, d)\n",
    "        self.register_buffer(\"mean_\", X.mean(0, keepdim=True))\n",
    "        Z = X - self.mean_ # center\n",
    "        U, S, Vh = torch.linalg.svd(Z, full_matrices=False)\n",
    "        Vt = Vh\n",
    "        U, Vt = svd_flip(U, Vt)\n",
    "        self.register_buffer(\"components_\", Vt[:d])\n",
    "        return self\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert hasattr(self, \"components_\"), \"PCA must be fit before use.\"\n",
    "        return torch.matmul(X - self.mean_, self.components_.t())\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, Y):\n",
    "        assert hasattr(self, \"components_\"), \"PCA must be fit before use.\"\n",
    "        return torch.matmul(Y, self.components_) + self.mean_\n",
    "\n",
    "def get_icv(model:HookedTransformer, positive_sentences, negative_sentences):\n",
    "    pos_tokens = model.to_tokens(positive_sentences, padding_side='left')\n",
    "    neg_tokens = model.to_tokens(negative_sentences, padding_side='left')\n",
    "    names_filter = lambda x : x.startswith(\"blocks.\") and x.endswith(ACT_NAME)\n",
    "    pos_logits, pos_cache = model.run_with_cache(pos_tokens, names_filter=names_filter)\n",
    "    neg_logits, neg_cache = model.run_with_cache(neg_tokens, names_filter=names_filter)\n",
    "    pos_vectors = einops.rearrange([pos_cache[utils.get_act_name(ACT_NAME, l)][:,-1,:] for l in range(model.cfg.n_layers)], 'l b d -> l b d')\n",
    "    neg_vectors = einops.rearrange([neg_cache[utils.get_act_name(ACT_NAME, l)][:,-1,:] for l in range(model.cfg.n_layers)], 'l b d -> l b d')\n",
    "    fit_data = einops.rearrange(pos_vectors - neg_vectors, 'l b d -> b (l d)')\n",
    "    pca = PCA(n_components=1).to(fit_data.device).fit(fit_data.float())\n",
    "    direction = (pca.components_.sum(dim=0,keepdim=True) + pca.mean_).mean(0)\n",
    "    icv = direction.view(model.cfg.n_layers, -1)\n",
    "    return icv\n",
    "\n",
    "def apply_icv(model:HookedTransformer, icv, lamb=0.1):\n",
    "    model.reset_hooks()\n",
    "    def residual_stream_edit_hook(\n",
    "        resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "        hook: HookPoint,\n",
    "        layer: int\n",
    "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
    "        original_norm = torch.norm(resid_pre, dim=-1, keepdim=True)\n",
    "        resid_pre += einops.repeat(icv[layer], 'd_model -> batch pos d_model', batch=resid_pre.shape[0], pos=resid_pre.shape[1]) * lamb\n",
    "        new_norm = torch.norm(resid_pre, dim=-1, keepdim=True)\n",
    "        resid_pre = resid_pre / new_norm * original_norm\n",
    "        return resid_pre\n",
    "    for l in range(model.cfg.n_layers):\n",
    "        model.blocks[l].hook_resid_pre.add_hook(partial(residual_stream_edit_hook, layer=l))\n",
    "    return model\n",
    "\n",
    "def compare_all(model:HookedTransformer, test_sentences, positive_sentences, negative_sentences):\n",
    "    model.reset_hooks()\n",
    "    zs_input = [PROMPT+FS_TEMPLATE.format(s,'') for s in test_sentences]\n",
    "    fs_examples = '\\n'.join([FS_TEMPLATE.format(s1,s2) for s1,s2 in zip(negative_sentences, positive_sentences)])\n",
    "    fs_input = [PROMPT+fs_examples+'\\n'+FS_TEMPLATE.format(s,'') for s in test_sentences]\n",
    "    \n",
    "    zs_tokens = model.to_tokens(zs_input, padding_side='left')\n",
    "    fs_tokens = model.to_tokens(fs_input, padding_side='left')\n",
    "    gen_configs = dict(max_new_tokens=20, temperature=0, top_k=1, top_p=1, do_sample=False)\n",
    "    \n",
    "    zs_output = model.generate(zs_tokens, **gen_configs)\n",
    "    zs_output = model.to_string(zs_output)\n",
    "    fs_output = model.generate(fs_tokens, **gen_configs)\n",
    "    fs_output = model.to_string(fs_output)\n",
    "        \n",
    "    icv = get_icv(model, positive_sentences, negative_sentences)\n",
    "    model = apply_icv(model, icv)\n",
    "    \n",
    "    icv_output = model.generate(zs_tokens, **gen_configs)\n",
    "    icv_output = model.to_string(icv_output)\n",
    "    pad_token = model.tokenizer.pad_token\n",
    "    \n",
    "    wash = lambda text, pattern : text.replace(pattern,'').replace(pad_token,'').strip()\n",
    "    \n",
    "    for zi, fi, zo, fo, io in zip(zs_input,fs_input, zs_output,fs_output,icv_output):\n",
    "        print(\"ZS PROMPT:\", zi)\n",
    "        print(\"FS PROMPT:\", fi)\n",
    "        print(f\"ZS:\\n\\n{wash(zo, zi)}\")\n",
    "        print(f\"FS:\\n\\n{wash(fo, fi)}\")\n",
    "        print(f\"ICV:\\n\\n{wash(io, zi)}\")\n",
    "        print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he was brave too!', 'It would have been good if he went to api. He would fit in.', \"I'm not gonna sell the picture, i just want to contribute to the article.\", 'the news that is being created by cnn and other news agencies is outrageous.', \"The reason they don't exist is because neither lies like trump\"]\n",
      "['he had steel balls too !', 'dude should have been taken to api , he would be right at home with all the other knuckleheads there', 'im not gonna sell the fucking picture , i just want to contribute to the fucking article .', 'the garbage that is being created by cnn and other news agencies is outrageous .', 'the reason they dont exist is because neither is a pathological liar like trump .']\n",
      "['in other news no one fucking cares .', \"hamas is winning this battle despite the gazan death toll because hamas doesn 't give two fucks about gazans .\", 'christians love to shit on the pope .', 'the shit her own country does is just as bad , if not worse .', 'at some point , the poor will get sick of this shit .', 'when they do shit like this .', 'but if saying \" fuck that group \" is much more offensive for you , go ahead .', 'it hurts how judgemental assholes view them , and no one gives a shit about that .', 'shit we probably literally blow that up in a week .', 'if anyone deserved it , it was this shit bag .']\n"
     ]
    }
   ],
   "source": [
    "num_shots = 5\n",
    "num_test = 10\n",
    "positive_sentences = dataset['train']['en_neutral_comment'][:num_shots]\n",
    "negative_sentences = dataset['train']['en_toxic_comment'][:num_shots]\n",
    "test_sentences = dataset['train']['en_toxic_comment'][-num_test:]\n",
    "print(positive_sentences)\n",
    "print(negative_sentences)\n",
    "print(test_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d238e932551e4e55b13b4bb83e80d8f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec4f9dc1ff242679ccdb88b407b44c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c831ca4baa5406496a0c1b57e724a4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:in other news no one fucking cares .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:in other news no one fucking cares .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  In other news, no one cares.\n",
      "I'm not sure if this is the best\n",
      "FS:\n",
      "\n",
      "<s> In other news, no one cares.\n",
      "Sentence:i dont care about the f\n",
      "ICV:\n",
      "\n",
      "<s> In other news, no one cares.\n",
      "I's a little bit of a stretch,\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:hamas is winning this battle despite the gazan death toll because hamas doesn 't give two fucks about gazans .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:hamas is winning this battle despite the gazan death toll because hamas doesn 't give two fucks about gazans .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s> Hamas is winning this battle despite the Gazan death toll because Hamas doesn't\n",
      "FS:\n",
      "\n",
      "<s> Hamas is winning this battle despite the gazan death toll because hamas doesn't\n",
      "ICV:\n",
      "\n",
      "<s> Ham is winning this battle despite the Gazan death tos because Ham doesn doesn's give\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:christians love to shit on the pope .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:christians love to shit on the pope .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s> Christians love to shit on the pope.\n",
      "I'm not sure if this is\n",
      "FS:\n",
      "\n",
      "<s> Christians love to shit on the pope.\n",
      "Sentence:i dont think tr\n",
      "ICV:\n",
      "\n",
      "<s> Christians love to shit on the Pope.\n",
      "I's a little bit of a stretch\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:the shit her own country does is just as bad , if not worse .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:the shit her own country does is just as bad , if not worse .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  The shit her own country does is just as bad , if not worse .\n",
      "I'm\n",
      "FS:\n",
      "\n",
      "<s> The shit her own country does is just as bad, if not worse.\n",
      "Sentence\n",
      "ICV:\n",
      "\n",
      "<s> The shit that the United States does is just as bad, if not worse, than the sh\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:at some point , the poor will get sick of this shit .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:at some point , the poor will get sick of this shit .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  At some point, the poor will get sick of this shit.\n",
      "The poor will get sick\n",
      "FS:\n",
      "\n",
      "<s> At some point, the poor will get sick of this shit.\n",
      "Sentence:i\n",
      "ICV:\n",
      "\n",
      "<s> At some point, the poor will get sick of this shit.\n",
      "I's a little\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:when they do shit like this .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:when they do shit like this .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  When they do shit like this, they are being stupid.\n",
      "Paraphrase: When\n",
      "FS:\n",
      "\n",
      "<s> When they do shit like this.\n",
      "Sentence:i dont think he should be president\n",
      "ICV:\n",
      "\n",
      "<s> When they do shit like this.\n",
      "I's a little bit of a stretch, but\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:but if saying \" fuck that group \" is much more offensive for you , go ahead .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:but if saying \" fuck that group \" is much more offensive for you , go ahead .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  but if saying \" fuck that group \" is much more offensive for you , go ahead .\n",
      "FS:\n",
      "\n",
      "<s> But if saying \"fuck that group\" is much more offensive for you, go ahead.\n",
      "ICV:\n",
      "\n",
      "<s> I's not a big fan of the group, but I'm not a big fan of\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:it hurts how judgemental assholes view them , and no one gives a shit about that .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:it hurts how judgemental assholes view them , and no one gives a shit about that .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  It hurts how judgemental assholes view them, and no one gives a sh\n",
      "FS:\n",
      "\n",
      "<s> It hurts how judgemental assholes view them, and no one gives a sh\n",
      "ICV:\n",
      "\n",
      "<s> It hurts how people view them, and no one cares.\n",
      "I's a little\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:shit we probably literally blow that up in a week .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:shit we probably literally blow that up in a week .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  We probably won't be able to use this for very long.\n",
      "Paraphrase:\n",
      "FS:\n",
      "\n",
      "<s> We probably blow that up in a week.\n",
      "Sentence:i dont think ive ever\n",
      "ICV:\n",
      "\n",
      "<s> I think we will probably blow up the bomb in a week.\n",
      "I's a little bit\n",
      "\n",
      "\n",
      "\n",
      "ZS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:if anyone deserved it , it was this shit bag .\n",
      "Paraphrase:\n",
      "FS PROMPT: Instruction: Please paraphrase the following sentence.\n",
      "Sentence:he had steel balls too !\n",
      "Paraphrase:he was brave too!\n",
      "Sentence:dude should have been taken to api , he would be right at home with all the other knuckleheads there\n",
      "Paraphrase:It would have been good if he went to api. He would fit in.\n",
      "Sentence:im not gonna sell the fucking picture , i just want to contribute to the fucking article .\n",
      "Paraphrase:I'm not gonna sell the picture, i just want to contribute to the article.\n",
      "Sentence:the garbage that is being created by cnn and other news agencies is outrageous .\n",
      "Paraphrase:the news that is being created by cnn and other news agencies is outrageous.\n",
      "Sentence:the reason they dont exist is because neither is a pathological liar like trump .\n",
      "Paraphrase:The reason they don't exist is because neither lies like trump\n",
      "Sentence:if anyone deserved it , it was this shit bag .\n",
      "Paraphrase:\n",
      "ZS:\n",
      "\n",
      "<s>  If anyone deserved it, it was this shit bag.\n",
      "I'm not sure if\n",
      "FS:\n",
      "\n",
      "<s> If anyone deserved it, it was this shit bag.\n",
      "Sentence:i dont\n",
      "ICV:\n",
      "\n",
      "<s> If anyone deserved it, it was this shit.\n",
      "I's a shit.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_all(model, test_sentences, positive_sentences, negative_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
