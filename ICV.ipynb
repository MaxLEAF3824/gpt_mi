{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:02:39.126219Z",
     "start_time": "2024-02-26T07:02:29.718158Z"
    }
   },
   "outputs": [],
   "source": [
    "# Unofficial Inplementation of In-Context-Vector with transformer_lens Library\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import (\n",
    "    HookPoint,\n",
    ")  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, FactoredMatrix\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "from fancy_einsum import einsum\n",
    "import tqdm.auto as tqdm\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from jaxtyping import Float\n",
    "from functools import partial\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:14:45.688416Z",
     "start_time": "2024-02-26T07:14:44.732126Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SH-IDC1-10-140-1-42       Mon Feb 26 15:14:45 2024  525.60.13\n",
      "[0] NVIDIA A100-SXM4-80GB | 59°C,  99 % | 74975 / 81920 MB | limo(74972M)\n",
      "[1] NVIDIA A100-SXM4-80GB | 44°C,  79 % | 20027 / 81920 MB | limo(20024M)\n",
      "[2] NVIDIA A100-SXM4-80GB | 51°C,  99 % | 74975 / 81920 MB | limo(74972M)\n",
      "[3] NVIDIA A100-SXM4-80GB | 57°C,  45 % | 35831 / 81920 MB | limo(35828M)\n",
      "[4] NVIDIA A100-SXM4-80GB | 39°C,   0 % |     0 / 81920 MB |\n",
      "[5] NVIDIA A100-SXM4-80GB | 41°C,  46 % | 39435 / 81920 MB | limo(39432M)\n",
      "[6] NVIDIA A100-SXM4-80GB | 41°C,  47 % | 39533 / 81920 MB | limo(39530M)\n",
      "[7] NVIDIA A100-SXM4-80GB | 51°C,  49 % | 35995 / 81920 MB | limo(35992M)\n"
     ]
    }
   ],
   "source": [
    "import gpustat\n",
    "gpustat.print_gpustat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:03:57.138338Z",
     "start_time": "2024-02-26T07:03:32.560538Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58247dd1ebe649ada585bd9ca4dd4c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/petrelfs/guoyiqiu/miniconda3/envs/mi/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "WARNING:root:float16 models may not work on CPU. Consider using a GPU or bfloat16.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model llama-7b-hf into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en_toxic_comment', 'en_neutral_comment'],\n",
       "        num_rows: 19744\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = os.path.join(os.environ['my_models_dir'], 'llama-7b')\n",
    "llama_7b_model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = HookedTransformer.from_pretrained_no_processing(\"llama-7b-hf\", hf_model=llama_7b_model, tokenizer=tokenizer, dtype='float16')\n",
    "# model = HookedTransformer.from_pretrained_no_processing(\"gpt2-xl\", dtype='float16')\n",
    "dataset = load_dataset(\"s-nlp/paradetox\")\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:11:16.381158Z",
     "start_time": "2024-02-26T07:11:16.370019Z"
    }
   },
   "outputs": [],
   "source": [
    "act_name = 'resid_pre'\n",
    "def svd_flip(u, v):\n",
    "    # columns of u, rows of v\n",
    "    max_abs_cols = torch.argmax(torch.abs(u), 0)\n",
    "    i = torch.arange(u.shape[1]).to(u.device)\n",
    "    signs = torch.sign(u[max_abs_cols, i])\n",
    "    u *= signs\n",
    "    v *= signs.view(-1, 1)\n",
    "    return u, v\n",
    "\n",
    "class PCA(nn.Module):\n",
    "    def __init__(self, n_components):\n",
    "        super().__init__()\n",
    "        self.n_components = n_components\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def fit(self, X):\n",
    "        n, d = X.size()\n",
    "        if self.n_components is not None:\n",
    "            d = min(self.n_components, d)\n",
    "        self.register_buffer(\"mean_\", X.mean(0, keepdim=True))\n",
    "        Z = X - self.mean_ # center\n",
    "        U, S, Vh = torch.linalg.svd(Z, full_matrices=False)\n",
    "        Vt = Vh\n",
    "        U, Vt = svd_flip(U, Vt)\n",
    "        self.register_buffer(\"components_\", Vt[:d])\n",
    "        return self\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        assert hasattr(self, \"components_\"), \"PCA must be fit before use.\"\n",
    "        return torch.matmul(X - self.mean_, self.components_.t())\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "\n",
    "    def inverse_transform(self, Y):\n",
    "        assert hasattr(self, \"components_\"), \"PCA must be fit before use.\"\n",
    "        return torch.matmul(Y, self.components_) + self.mean_\n",
    "\n",
    "def get_in_context_vectors(model:HookedTransformer, positive_sentences, negative_sentences):\n",
    "    pos_tokens = model.to_tokens(positive_sentences)\n",
    "    neg_tokens = model.to_tokens(negative_sentences)\n",
    "    names_filter = lambda x : x.startswith(\"blocks.\") and x.endswith(act_name)\n",
    "    pos_logits, pos_cache = model.run_with_cache(pos_tokens, names_filter=names_filter)\n",
    "    neg_logits, neg_cache = model.run_with_cache(neg_tokens, names_filter=names_filter)\n",
    "    pos_vectors = einops.rearrange([pos_cache[utils.get_act_name(act_name, l)][:,-1,:] for l in range(model.cfg.n_layers)], 'l b d -> b (l d)')\n",
    "    neg_vectors = einops.rearrange([neg_cache[utils.get_act_name(act_name, l)][:,-1,:] for l in range(model.cfg.n_layers)], 'l b d -> b (l d)')\n",
    "    fit_data = pos_vectors - neg_vectors\n",
    "    pca = PCA(n_components=1).to(fit_data.device).fit(fit_data.float())\n",
    "    direction = (pca.components_.sum(dim=0,keepdim=True) + pca.mean_).mean(0)\n",
    "    icv = direction.view(model.cfg.n_layers, -1)\n",
    "    return icv\n",
    "\n",
    "def display_icv(model:HookedTransformer, icv, input, lamb=0.1):\n",
    "    model.reset_hooks()\n",
    "    original_output = model.generate(input, max_new_tokens=20, temperature=0, top_k=1, top_p=1, do_sample=False)\n",
    "    def residual_stream_edit_hook(\n",
    "        resid_pre: Float[torch.Tensor, \"batch pos d_model\"],\n",
    "        hook: HookPoint,\n",
    "        layer: int\n",
    "    ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n",
    "        original_norm = torch.norm(resid_pre, dim=-1, keepdim=True)\n",
    "        resid_pre += einops.repeat(icv[layer], 'd_model -> batch pos d_model', batch=resid_pre.shape[0], pos=resid_pre.shape[1]) * lamb\n",
    "        new_norm = torch.norm(resid_pre, dim=-1, keepdim=True)\n",
    "        resid_pre = resid_pre / new_norm * original_norm\n",
    "        return resid_pre\n",
    "    for l in range(model.cfg.n_layers):\n",
    "        model.blocks[l].hook_resid_pre.add_hook(partial(residual_stream_edit_hook, layer=l))\n",
    "    modified_output = model.generate(input, max_new_tokens=20, temperature=0, top_k=1, top_p=1, do_sample=False)\n",
    "    print(f\"Original: \\n\\n{original_output}\\nModified: \\n\\n{modified_output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:11:24.601264Z",
     "start_time": "2024-02-26T07:11:21.994271Z"
    }
   },
   "outputs": [],
   "source": [
    "num_shots = 5\n",
    "positive_sentences = dataset['train']['en_neutral_comment'][:num_shots]\n",
    "negative_sentences = dataset['train']['en_toxic_comment'][:num_shots]\n",
    "icv = get_in_context_vectors(model, positive_sentences, negative_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-26T07:11:28.592521Z",
     "start_time": "2024-02-26T07:11:26.246381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input:  Instruction: Please paraphrase the following sentence.\n",
      "Sentence:if anyone deserved it , it was this shit bag .\n",
      "Paraphrase:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e0982f0f0a4076abbb01c5b27070e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79b5245845404ab480463a70676117b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: \n",
      "\n",
      "Instruction: Please paraphrase the following sentence.\n",
      "Sentence:if anyone deserved it , it was this shit bag .\n",
      "Paraphrase: If anyone deserved it, it was this shit bag.\n",
      "I'm not sure if\n",
      "Modified: \n",
      "\n",
      "Instruction: Please paraphrase the following sentence.\n",
      "Sentence:if anyone deserved it , it was this shit bag .\n",
      "Paraphrase:textttextttextttextttextttextttextttextttextttextttextttextttextttextttextttextttextttextttextttextt\n"
     ]
    }
   ],
   "source": [
    "input_template = \"Instruction: Please paraphrase the following sentence.\\nSentence:{}\\nParaphrase:\"\n",
    "test_sentence = dataset['train']['en_toxic_comment'][-1]\n",
    "input=input_template.format(test_sentence)\n",
    "print('input: ', input)\n",
    "display_icv(model, icv, input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
